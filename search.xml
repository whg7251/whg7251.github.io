<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Maxwell读取MySQL binlog日志到Kafka</title>
      <link href="/2019/11/23/Maxwell%E8%AF%BB%E5%8F%96MySQL%20binlog%E6%97%A5%E5%BF%97%E5%88%B0Kafka/"/>
      <url>/2019/11/23/Maxwell%E8%AF%BB%E5%8F%96MySQL%20binlog%E6%97%A5%E5%BF%97%E5%88%B0Kafka/</url>
      
        <content type="html"><![CDATA[<h1 id="Maxwell读取MySQL-binlog日志到Kafka"><a href="#Maxwell读取MySQL-binlog日志到Kafka" class="headerlink" title="Maxwell读取MySQL binlog日志到Kafka"></a>Maxwell读取MySQL binlog日志到Kafka</h1><h3 id="启动MySQL"><a href="#启动MySQL" class="headerlink" title="启动MySQL"></a>启动MySQL</h3><h3 id="创建maxwell的数据库和用户"><a href="#创建maxwell的数据库和用户" class="headerlink" title="创建maxwell的数据库和用户"></a>创建maxwell的数据库和用户</h3><h3 id="在MySQL中创建一个测试数据库和表"><a href="#在MySQL中创建一个测试数据库和表" class="headerlink" title="在MySQL中创建一个测试数据库和表"></a>在MySQL中创建一个测试数据库和表</h3><p>前面三个步骤详见 <a href="https://blog.51cto.com/14309075/2415503" target="_blank" rel="noopener">Maxwell读取MySQL binlog日志通过stdout展示</a></p><h3 id="启动Zookeeper"><a href="#启动Zookeeper" class="headerlink" title="启动Zookeeper"></a>启动Zookeeper</h3><pre><code class="shell">[hadoop@hadoop001 ~]$ cd $ZK_HOME/bin[hadoop@hadoop001 bin]$ ./zkServer.sh start</code></pre><h3 id="启动kafka，并创建主题为maxwell的topic"><a href="#启动kafka，并创建主题为maxwell的topic" class="headerlink" title="启动kafka，并创建主题为maxwell的topic"></a>启动kafka，并创建主题为maxwell的topic</h3><pre><code class="shell">[hadoop@hadoop001 bin]$ cd $KAFKA_HOME//查看kafka版本，防止maxwell不支持[hadoop@hadoop001 kafka]$ find ./libs/ -name \*kafka_\* | head -1 | grep -o &#39;\kafka[^\n]*&#39;kafka_2.11-0.10.0.1-sources.jar//启动kafka-server服务[hadoop@hadoop001 kafka]$ nohup bin/kafka-server-start.sh config/server.properties &amp;[hadoop@hadoop001 kafka]$ jps13460 QuorumPeerMain14952 Jps13518 Kafka[hadoop@hadoop001 kafka]$ bin/kafka-topics.sh --create --zookeeper 192.168.137.2:2181/kafka --replication-factor 1 --partitions 3 --topic maxwellCreated topic &quot;maxwell&quot;.[hadoop@hadoop001 kafka]$ bin/kafka-topics.sh --list --zookeeper 192.168.137.2:2181/kafka__consumer_offsetsmaxwell</code></pre><h3 id="启动kafaka的消费者，检查数据是否到位"><a href="#启动kafaka的消费者，检查数据是否到位" class="headerlink" title="启动kafaka的消费者，检查数据是否到位"></a>启动kafaka的消费者，检查数据是否到位</h3><pre><code class="shell">[hadoop@hadoop001 kafka]$ bin/kafka-console-consumer.sh --zookeeper 192.168.137.2:2181/kafka --topic maxwell --from-beginning</code></pre><h3 id="启动maxwell进程"><a href="#启动maxwell进程" class="headerlink" title="启动maxwell进程"></a>启动maxwell进程</h3><pre><code class="shell">//先检查maxwell是否支持kafka-0.10.0.1[root@hadoop000 ~]# cd /root/app/maxwell-1.17.1/lib/kafka-clients[root@hadoop001 kafka-clients]# lltotal 5556-rw-r--r-- 1 yarn games  746207 Jul  3  2018 kafka-clients-0.10.0.1.jar-rw-r--r-- 1 yarn games  951041 Jul  3  2018 kafka-clients-0.10.2.1.jar-rw-r--r-- 1 yarn games 1419544 Jul  3  2018 kafka-clients-0.11.0.1.jar-rw-r--r-- 1 yarn games  324016 Jul  3  2018 kafka-clients-0.8.2.2.jar-rw-r--r-- 1 yarn games  641408 Jul  3  2018 kafka-clients-0.9.0.1.jar-rw-r--r-- 1 yarn games 1591338 Jul  3  2018 kafka-clients-1.0.0.jar//发现支持kafka-0.10.0.1版本，假如没有生产上正在用的kafka版本的jar包，可以直接把这个版本的client jar包copy进来//启动maxwell[root@hadoop001 maxwell-1.17.1]# bin/maxwell --user=&#39;maxwell&#39; --password=&#39;maxwell&#39; --host=&#39;127.0.0.1&#39; --producer=kafka --kafka_version=0.10.0.1 --kafka.bootstrap.servers=192.168.137.2:9092 --kafka_topic=maxwellUsing kafka version: 0.10.0.110:16:52,979 WARN  MaxwellMetrics - Metrics will not be exposed: metricsReportingType not configured.10:16:53,451 INFO  ProducerConfig - ProducerConfig values:         metric.reporters = []        metadata.max.age.ms = 300000        reconnect.backoff.ms = 50        sasl.kerberos.ticket.renew.window.factor = 0.8        bootstrap.servers = [192.168.137.2:9092]        ssl.keystore.type = JKS        sasl.mechanism = GSSAPI        max.block.ms = 60000        interceptor.classes = null        ssl.truststore.password = null        client.id =         ssl.endpoint.identification.algorithm = null        request.timeout.ms = 30000        acks = 1        receive.buffer.bytes = 32768        ssl.truststore.type = JKS        retries = 0        ssl.truststore.location = null        ssl.keystore.password = null        send.buffer.bytes = 131072        compression.type = none        metadata.fetch.timeout.ms = 60000        retry.backoff.ms = 100        sasl.kerberos.kinit.cmd = /usr/bin/kinit        buffer.memory = 33554432        timeout.ms = 30000        key.serializer = class org.apache.kafka.common.serialization.StringSerializer        sasl.kerberos.service.name = null        sasl.kerberos.ticket.renew.jitter = 0.05        ssl.trustmanager.algorithm = PKIX        block.on.buffer.full = false        ssl.key.password = null        sasl.kerberos.min.time.before.relogin = 60000        connections.max.idle.ms = 540000        max.in.flight.requests.per.connection = 5        metrics.num.samples = 2        ssl.protocol = TLS        ssl.provider = null        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]        batch.size = 16384        ssl.keystore.location = null        ssl.cipher.suites = null        security.protocol = PLAINTEXT        max.request.size = 1048576        value.serializer = class org.apache.kafka.common.serialization.StringSerializer        ssl.keymanager.algorithm = SunX509        metrics.sample.window.ms = 30000        partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner        linger.ms = 010:16:53,512 INFO  ProducerConfig - ProducerConfig values:         metric.reporters = []        metadata.max.age.ms = 300000        reconnect.backoff.ms = 50        sasl.kerberos.ticket.renew.window.factor = 0.8        bootstrap.servers = [192.168.137.2:9092]        ssl.keystore.type = JKS        sasl.mechanism = GSSAPI        max.block.ms = 60000        interceptor.classes = null        ssl.truststore.password = null        client.id = producer-1        ssl.endpoint.identification.algorithm = null        request.timeout.ms = 30000        acks = 1        receive.buffer.bytes = 32768        ssl.truststore.type = JKS        retries = 0        ssl.truststore.location = null        ssl.keystore.password = null        send.buffer.bytes = 131072        compression.type = none        metadata.fetch.timeout.ms = 60000        retry.backoff.ms = 100        sasl.kerberos.kinit.cmd = /usr/bin/kinit        buffer.memory = 33554432        timeout.ms = 30000        key.serializer = class org.apache.kafka.common.serialization.StringSerializer        sasl.kerberos.service.name = null        sasl.kerberos.ticket.renew.jitter = 0.05        ssl.trustmanager.algorithm = PKIX        block.on.buffer.full = false        ssl.key.password = null        sasl.kerberos.min.time.before.relogin = 60000        connections.max.idle.ms = 540000        max.in.flight.requests.per.connection = 5        metrics.num.samples = 2        ssl.protocol = TLS        ssl.provider = null        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]        batch.size = 16384        ssl.keystore.location = null        ssl.cipher.suites = null        security.protocol = PLAINTEXT        max.request.size = 1048576        value.serializer = class org.apache.kafka.common.serialization.StringSerializer        ssl.keymanager.algorithm = SunX509        metrics.sample.window.ms = 30000        partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner        linger.ms = 010:16:53,516 INFO  AppInfoParser - Kafka version : 0.10.0.110:16:53,516 INFO  AppInfoParser - Kafka commitId : a7a17cdec9eaa6c510:16:53,550 INFO  Maxwell - Maxwell v1.17.1 is booting (MaxwellKafkaProducer), starting at Position[BinlogPosition[mysql-bin.000016:116360], lastHeartbeat=1552092988288]10:16:53,730 INFO  MysqlSavedSchema - Restoring schema id 1 (last modified at Position[BinlogPosition[mysql-bin.000014:5999], lastHeartbeat=0])10:16:53,846 INFO  BinlogConnectorReplicator - Setting initial binlog pos to: mysql-bin.000016:11636010:16:53,951 INFO  BinaryLogClient - Connected to 127.0.0.1:3306 at mysql-bin.000016/116360 (sid:6379, cid:4)10:16:53,951 INFO  BinlogConnectorLifecycleListener - Binlog connected.</code></pre><h3 id="在MySQL中更新一条数据"><a href="#在MySQL中更新一条数据" class="headerlink" title="在MySQL中更新一条数据"></a>在MySQL中更新一条数据</h3><pre><code class="sql">mysql&gt; update emp set sal=502 where empno=6001;mysql&gt; update emp set sal=603 where empno=6001;mysql&gt; create table emp1 select * from emp;</code></pre><h3 id="查看kafka的消费者"><a href="#查看kafka的消费者" class="headerlink" title="查看kafka的消费者"></a>查看kafka的消费者</h3><pre><code class="json">//对应第一条insert语句{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;update&quot;,&quot;ts&quot;:1552097863,&quot;xid&quot;:89,&quot;commit&quot;:true,&quot;data&quot;:{&quot;empno&quot;:6001,&quot;ename&quot;:&quot;SIWA&quot;,&quot;job&quot;:&quot;DESIGNER&quot;,&quot;mgr&quot;:7001,&quot;hiredate&quot;:&quot;2019-03-08 00:00:00&quot;,&quot;sal&quot;:502.00,&quot;comm&quot;:6000.00,&quot;deptno&quot;:40},&quot;old&quot;:{&quot;sal&quot;:501.00}}//对应第二条insert语句{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;update&quot;,&quot;ts&quot;:1552097951,&quot;xid&quot;:123,&quot;commit&quot;:true,&quot;data&quot;:{&quot;empno&quot;:6001,&quot;ename&quot;:&quot;SIWA&quot;,&quot;job&quot;:&quot;DESIGNER&quot;,&quot;mgr&quot;:7001,&quot;hiredate&quot;:&quot;2019-03-08 00:00:00&quot;,&quot;sal&quot;:603.00,&quot;comm&quot;:6000.00,&quot;deptno&quot;:40},&quot;old&quot;:{&quot;sal&quot;:502.00}}//对应建表，相当于在新表emp1里插入了emp表里的所有数据{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:0,&quot;data&quot;:{&quot;empno&quot;:6001,&quot;ename&quot;:&quot;SIWA&quot;,&quot;job&quot;:&quot;DESIGNER&quot;,&quot;mgr&quot;:7001,&quot;hiredate&quot;:&quot;2019-03-08 00:00:00&quot;,&quot;sal&quot;:603.00,&quot;comm&quot;:6000.00,&quot;deptno&quot;:40}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:1,&quot;data&quot;:{&quot;empno&quot;:7369,&quot;ename&quot;:&quot;SMITH&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7902,&quot;hiredate&quot;:&quot;1980-12-17 00:00:00&quot;,&quot;sal&quot;:800.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:2,&quot;data&quot;:{&quot;empno&quot;:7499,&quot;ename&quot;:&quot;ALLEN&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-02-20 00:00:00&quot;,&quot;sal&quot;:1600.00,&quot;comm&quot;:300.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:3,&quot;data&quot;:{&quot;empno&quot;:7521,&quot;ename&quot;:&quot;WARD&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-02-22 00:00:00&quot;,&quot;sal&quot;:1250.00,&quot;comm&quot;:500.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:4,&quot;data&quot;:{&quot;empno&quot;:7566,&quot;ename&quot;:&quot;JONES&quot;,&quot;job&quot;:&quot;MANAGER&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1981-04-02 00:00:00&quot;,&quot;sal&quot;:2975.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:5,&quot;data&quot;:{&quot;empno&quot;:7654,&quot;ename&quot;:&quot;MARTIN&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-09-28 00:00:00&quot;,&quot;sal&quot;:1250.00,&quot;comm&quot;:1400.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:6,&quot;data&quot;:{&quot;empno&quot;:7698,&quot;ename&quot;:&quot;BLAKE&quot;,&quot;job&quot;:&quot;MANAGER&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1981-05-01 00:00:00&quot;,&quot;sal&quot;:2850.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:7,&quot;data&quot;:{&quot;empno&quot;:7782,&quot;ename&quot;:&quot;CLARK&quot;,&quot;job&quot;:&quot;MANAGER&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1981-06-09 00:00:00&quot;,&quot;sal&quot;:2450.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:10}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:8,&quot;data&quot;:{&quot;empno&quot;:7788,&quot;ename&quot;:&quot;SCOTT&quot;,&quot;job&quot;:&quot;ANALYST&quot;,&quot;mgr&quot;:7566,&quot;hiredate&quot;:&quot;1987-04-19 00:00:00&quot;,&quot;sal&quot;:3000.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:9,&quot;data&quot;:{&quot;empno&quot;:7839,&quot;ename&quot;:&quot;KING&quot;,&quot;job&quot;:&quot;PRESIDENT&quot;,&quot;mgr&quot;:0,&quot;hiredate&quot;:&quot;1981-11-17 00:00:00&quot;,&quot;sal&quot;:5000.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:10}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:10,&quot;data&quot;:{&quot;empno&quot;:7844,&quot;ename&quot;:&quot;TURNER&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-09-08 00:00:00&quot;,&quot;sal&quot;:1500.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:11,&quot;data&quot;:{&quot;empno&quot;:7876,&quot;ename&quot;:&quot;ADAMS&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7788,&quot;hiredate&quot;:&quot;1987-05-23 00:00:00&quot;,&quot;sal&quot;:1100.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:12,&quot;data&quot;:{&quot;empno&quot;:7900,&quot;ename&quot;:&quot;JAMES&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-12-03 00:00:00&quot;,&quot;sal&quot;:950.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:13,&quot;data&quot;:{&quot;empno&quot;:7902,&quot;ename&quot;:&quot;FORD&quot;,&quot;job&quot;:&quot;ANALYST&quot;,&quot;mgr&quot;:7566,&quot;hiredate&quot;:&quot;1981-12-03 00:00:00&quot;,&quot;sal&quot;:3000.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:14,&quot;data&quot;:{&quot;empno&quot;:7934,&quot;ename&quot;:&quot;MILLER&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7782,&quot;hiredate&quot;:&quot;1982-01-23 00:00:00&quot;,&quot;sal&quot;:1300.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:10}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;commit&quot;:true,&quot;data&quot;:{&quot;empno&quot;:8888,&quot;ename&quot;:&quot;HIVE&quot;,&quot;job&quot;:&quot;PROGRAM&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1988-01-23 00:00:00&quot;,&quot;sal&quot;:10300.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:null}}# 数据已经正常同步到kafka中</code></pre><h3 id="Maxwell的过滤功能"><a href="#Maxwell的过滤功能" class="headerlink" title="Maxwell的过滤功能"></a>Maxwell的过滤功能</h3><p>参考过滤配置： &lt;<a href="http://maxwells-daemon.io/filtering/%3E" target="_blank" rel="noopener">http://maxwells-daemon.io/filtering/%3E</a>;</p><pre><code class="shell">[root@hadoop001 maxwell-1.17.1]# bin/maxwell --user=&#39;maxwell&#39; --password=&#39;maxwell&#39; \--host=&#39;127.0.0.1&#39; --filter &#39;exclude: *.*, include:hlwtest.emp1&#39; \--producer=kafka --kafka_version=0.10.0.1 --kafka.bootstrap.servers=192.168.137.2:9092 --kafka_topic=maxwell--filter &#39;exclude: *.*, include:hlwtest.emp1&#39;的意思是只监控hlwtest.emp1表的变化，其他的都不监控//MySQL中update数据mysql&gt; update emp set sal=730 where empno=6001;mysql&gt; update emp1 set sal=330 where empno=6001;mysql&gt; update emp set sal=730 where empno=6001;mysql&gt; update emp1 set sal=331 where empno=6001;//Kafka消费者接收到的数据{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;update&quot;,&quot;ts&quot;:1552099858,&quot;xid&quot;:916,&quot;commit&quot;:true,&quot;data&quot;:{&quot;empno&quot;:6001,&quot;ename&quot;:&quot;SIWA&quot;,&quot;job&quot;:&quot;DESIGNER&quot;,&quot;mgr&quot;:7001,&quot;hiredate&quot;:&quot;2019-03-08 00:00:00&quot;,&quot;sal&quot;:330.00,&quot;comm&quot;:6000.00,&quot;deptno&quot;:40},&quot;old&quot;:{&quot;sal&quot;:321.00}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;update&quot;,&quot;ts&quot;:1552099858,&quot;xid&quot;:922,&quot;commit&quot;:true,&quot;data&quot;:{&quot;empno&quot;:6001,&quot;ename&quot;:&quot;SIWA&quot;,&quot;job&quot;:&quot;DESIGNER&quot;,&quot;mgr&quot;:7001,&quot;hiredate&quot;:&quot;2019-03-08 00:00:00&quot;,&quot;sal&quot;:331.00,&quot;comm&quot;:6000.00,&quot;deptno&quot;:40},&quot;old&quot;:{&quot;sal&quot;:330.00}}确实只消费到了emp1表的update语句，而没有接收到emp表的更新</code></pre><h3 id="Maxwell-的bootstrap"><a href="#Maxwell-的bootstrap" class="headerlink" title="Maxwell 的bootstrap"></a>Maxwell 的bootstrap</h3><pre><code class="sql">mysql&gt; insert into maxwell.bootstrap (database_name, table_name) values (&quot;hlwtest&quot;, &quot;emp&quot;);mysql&gt; select * from maxwell.bootstrap;+----+---------------+------------+--------------+-------------+---------------+------------+------------+---------------------+---------------------+------------------+-----------------+-----------+| id | database_name | table_name | where_clause | is_complete | inserted_rows | total_rows | created_at | started_at          | completed_at        | binlog_file      | binlog_position | client_id |+----+---------------+------------+--------------+-------------+---------------+------------+------------+---------------------+---------------------+------------------+-----------------+-----------+|  1 | hlwtest       | emp        | NULL         |           1 |            16 |          0 | NULL       | 2019-03-09 11:33:11 | 2019-03-09 11:33:11 | mysql-bin.000018 |          225498 | maxwell   |+----+---------------+------------+--------------+-------------+---------------+------------+------------+---------------------+---------------------+------------------+-----------------+-----------+</code></pre><h3 id="kafka的消费窗口"><a href="#kafka的消费窗口" class="headerlink" title="kafka的消费窗口"></a>kafka的消费窗口</h3><pre><code class="json">{&quot;database&quot;:&quot;maxwell&quot;,&quot;table&quot;:&quot;bootstrap&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552102248,&quot;xid&quot;:1555,&quot;commit&quot;:true,&quot;data&quot;:{&quot;id&quot;:1,&quot;database_name&quot;:&quot;hlwtest&quot;,&quot;table_name&quot;:&quot;emp&quot;,&quot;where_clause&quot;:null,&quot;is_complete&quot;:0,&quot;inserted_rows&quot;:0,&quot;total_rows&quot;:0,&quot;created_at&quot;:null,&quot;started_at&quot;:null,&quot;completed_at&quot;:null,&quot;binlog_file&quot;:null,&quot;binlog_position&quot;:0,&quot;client_id&quot;:&quot;maxwell&quot;}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-start&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:6001,&quot;ename&quot;:&quot;SIWA&quot;,&quot;job&quot;:&quot;DESIGNER&quot;,&quot;mgr&quot;:7001,&quot;hiredate&quot;:&quot;2019-03-08 00:00:00&quot;,&quot;sal&quot;:730.00,&quot;comm&quot;:6000.00,&quot;deptno&quot;:40}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7369,&quot;ename&quot;:&quot;SMITH&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7902,&quot;hiredate&quot;:&quot;1980-12-17 00:00:00&quot;,&quot;sal&quot;:800.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7499,&quot;ename&quot;:&quot;ALLEN&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-02-20 00:00:00&quot;,&quot;sal&quot;:1600.00,&quot;comm&quot;:300.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7521,&quot;ename&quot;:&quot;WARD&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-02-22 00:00:00&quot;,&quot;sal&quot;:1250.00,&quot;comm&quot;:500.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7566,&quot;ename&quot;:&quot;JONES&quot;,&quot;job&quot;:&quot;MANAGER&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1981-04-02 00:00:00&quot;,&quot;sal&quot;:2975.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7654,&quot;ename&quot;:&quot;MARTIN&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-09-28 00:00:00&quot;,&quot;sal&quot;:1250.00,&quot;comm&quot;:1400.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7698,&quot;ename&quot;:&quot;BLAKE&quot;,&quot;job&quot;:&quot;MANAGER&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1981-05-01 00:00:00&quot;,&quot;sal&quot;:2850.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7782,&quot;ename&quot;:&quot;CLARK&quot;,&quot;job&quot;:&quot;MANAGER&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1981-06-09 00:00:00&quot;,&quot;sal&quot;:2450.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:10}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7788,&quot;ename&quot;:&quot;SCOTT&quot;,&quot;job&quot;:&quot;ANALYST&quot;,&quot;mgr&quot;:7566,&quot;hiredate&quot;:&quot;1987-04-19 00:00:00&quot;,&quot;sal&quot;:3000.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7839,&quot;ename&quot;:&quot;KING&quot;,&quot;job&quot;:&quot;PRESIDENT&quot;,&quot;mgr&quot;:0,&quot;hiredate&quot;:&quot;1981-11-17 00:00:00&quot;,&quot;sal&quot;:5000.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:10}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7844,&quot;ename&quot;:&quot;TURNER&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-09-08 00:00:00&quot;,&quot;sal&quot;:1500.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7876,&quot;ename&quot;:&quot;ADAMS&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7788,&quot;hiredate&quot;:&quot;1987-05-23 00:00:00&quot;,&quot;sal&quot;:1100.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7900,&quot;ename&quot;:&quot;JAMES&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-12-03 00:00:00&quot;,&quot;sal&quot;:950.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7902,&quot;ename&quot;:&quot;FORD&quot;,&quot;job&quot;:&quot;ANALYST&quot;,&quot;mgr&quot;:7566,&quot;hiredate&quot;:&quot;1981-12-03 00:00:00&quot;,&quot;sal&quot;:3000.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7934,&quot;ename&quot;:&quot;MILLER&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7782,&quot;hiredate&quot;:&quot;1982-01-23 00:00:00&quot;,&quot;sal&quot;:1300.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:10}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:8888,&quot;ename&quot;:&quot;HIVE&quot;,&quot;job&quot;:&quot;PROGRAM&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1988-01-23 00:00:00&quot;,&quot;sal&quot;:10300.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:null}}{&quot;database&quot;:&quot;maxwell&quot;,&quot;table&quot;:&quot;bootstrap&quot;,&quot;type&quot;:&quot;update&quot;,&quot;ts&quot;:1552102391,&quot;xid&quot;:1617,&quot;commit&quot;:true,&quot;data&quot;:{&quot;id&quot;:1,&quot;database_name&quot;:&quot;hlwtest&quot;,&quot;table_name&quot;:&quot;emp&quot;,&quot;where_clause&quot;:null,&quot;is_complete&quot;:1,&quot;inserted_rows&quot;:16,&quot;total_rows&quot;:0,&quot;created_at&quot;:null,&quot;started_at&quot;:&quot;2019-03-09 11:33:11&quot;,&quot;completed_at&quot;:&quot;2019-03-09 11:33:11&quot;,&quot;binlog_file&quot;:&quot;mysql-bin.000018&quot;,&quot;binlog_position&quot;:225498,&quot;client_id&quot;:&quot;maxwell&quot;},&quot;old&quot;:{&quot;is_complete&quot;:0,&quot;inserted_rows&quot;:1,&quot;completed_at&quot;:null}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-complete&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{}}</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>生产SparkStreaming数据零丢失最佳实践(含代码)</title>
      <link href="/2019/11/23/%E7%94%9F%E4%BA%A7SparkStreaming%E6%95%B0%E6%8D%AE%E9%9B%B6%E4%B8%A2%E5%A4%B1%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E4%BB%A3%E7%A0%81)/"/>
      <url>/2019/11/23/%E7%94%9F%E4%BA%A7SparkStreaming%E6%95%B0%E6%8D%AE%E9%9B%B6%E4%B8%A2%E5%A4%B1%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E4%BB%A3%E7%A0%81)/</url>
      
        <content type="html"><![CDATA[<h2 id="MySQL创建存储offset的表格"><a href="#MySQL创建存储offset的表格" class="headerlink" title="MySQL创建存储offset的表格"></a>MySQL创建存储offset的表格</h2><pre><code class="sql">mysql&gt; use testmysql&gt; create table hlw_offset(        topic varchar(32),        groupid varchar(50),        partitions int,        fromoffset bigint,        untiloffset bigint,        primary key(topic,groupid,partitions)        );</code></pre><h2 id="Maven依赖包"><a href="#Maven依赖包" class="headerlink" title="Maven依赖包"></a>Maven依赖包</h2><pre><code class="xml">&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;&lt;spark.version&gt;2.3.1&lt;/spark.version&gt;&lt;scalikejdbc.version&gt;2.5.0&lt;/scalikejdbc.version&gt;--------------------------------------------------&lt;dependency&gt;    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;    &lt;version&gt;${scala.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;    &lt;version&gt;${spark.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;    &lt;version&gt;${spark.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;    &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;    &lt;version&gt;${spark.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;    &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt;    &lt;version&gt;${spark.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;mysql&lt;/groupId&gt;    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;    &lt;version&gt;5.1.27&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.scalikejdbc/scalikejdbc --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt;    &lt;artifactId&gt;scalikejdbc_2.11&lt;/artifactId&gt;    &lt;version&gt;2.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt;    &lt;artifactId&gt;scalikejdbc-config_2.11&lt;/artifactId&gt;    &lt;version&gt;2.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;com.typesafe&lt;/groupId&gt;    &lt;artifactId&gt;config&lt;/artifactId&gt;    &lt;version&gt;1.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;    &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;    &lt;version&gt;3.5&lt;/version&gt;&lt;/dependency&gt;</code></pre><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><ol><li>StreamingContext</li><li>从kafka中获取数据(从外部存储获取offset–&gt;根据offset获取kafka中的数据)</li><li>根据业务进行逻辑处理</li><li>将处理结果存到外部存储中–保存offset</li><li>启动程序，等待程序结束</li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ol><li><p>SparkStreaming主体代码如下</p><pre><code class="scala">import kafka.common.TopicAndPartitionimport kafka.message.MessageAndMetadataimport kafka.serializer.StringDecoderimport org.apache.spark.SparkConfimport org.apache.spark.streaming.kafka.{HasOffsetRanges, KafkaUtils}import org.apache.spark.streaming.{Seconds, StreamingContext}import scalikejdbc._import scalikejdbc.config._object JDBCOffsetApp { def main(args: Array[String]): Unit = {   //创建SparkStreaming入口   val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;JDBCOffsetApp&quot;)   val ssc = new StreamingContext(conf,Seconds(5))   //kafka消费主题   val topics = ValueUtils.getStringValue(&quot;kafka.topics&quot;).split(&quot;,&quot;).toSet   //kafka参数   //这里应用了自定义的ValueUtils工具类，来获取application.conf里的参数，方便后期修改   val kafkaParams = Map[String,String](     &quot;metadata.broker.list&quot;-&gt;ValueUtils.getStringValue(&quot;metadata.broker.list&quot;),     &quot;auto.offset.reset&quot;-&gt;ValueUtils.getStringValue(&quot;auto.offset.reset&quot;),     &quot;group.id&quot;-&gt;ValueUtils.getStringValue(&quot;group.id&quot;)   )   //先使用scalikejdbc从MySQL数据库中读取offset信息   //+------------+------------------+------------+------------+-------------+   //| topic      | groupid          | partitions | fromoffset | untiloffset |   //+------------+------------------+------------+------------+-------------+   //MySQL表结构如上，将“topic”，“partitions”，“untiloffset”列读取出来   //组成 fromOffsets: Map[TopicAndPartition, Long]，后面createDirectStream用到   DBs.setup()   val fromOffset = DB.readOnly( implicit session =&gt; {     SQL(&quot;select * from hlw_offset&quot;).map(rs =&gt; {       (TopicAndPartition(rs.string(&quot;topic&quot;),rs.int(&quot;partitions&quot;)),rs.long(&quot;untiloffset&quot;))     }).list().apply()   }).toMap   //如果MySQL表中没有offset信息，就从0开始消费；如果有，就从已经存在的offset开始消费     val messages = if (fromOffset.isEmpty) {       println(&quot;从头开始消费...&quot;)       KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder](ssc,kafkaParams,topics)     } else {       println(&quot;从已存在记录开始消费...&quot;)       val messageHandler = (mm:MessageAndMetadata[String,String]) =&gt; (mm.key(),mm.message())       KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder,(String,String)](ssc,kafkaParams,fromOffset,messageHandler)     }     messages.foreachRDD(rdd=&gt;{       if(!rdd.isEmpty()){         //输出rdd的数据量         println(&quot;数据统计记录为：&quot;+rdd.count())         //官方案例给出的获得rdd offset信息的方法，offsetRanges是由一系列offsetRange组成的数组//          trait HasOffsetRanges {//            def offsetRanges: Array[OffsetRange]//          }         val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges         offsetRanges.foreach(x =&gt; {           //输出每次消费的主题，分区，开始偏移量和结束偏移量           println(s&quot;---${x.topic},${x.partition},${x.fromOffset},${x.untilOffset}---&quot;)          //将最新的偏移量信息保存到MySQL表中           DB.autoCommit( implicit session =&gt; {             SQL(&quot;replace into hlw_offset(topic,groupid,partitions,fromoffset,untiloffset) values (?,?,?,?,?)&quot;)           .bind(x.topic,ValueUtils.getStringValue(&quot;group.id&quot;),x.partition,x.fromOffset,x.untilOffset)             .update().apply()           })         })       }     })   ssc.start()   ssc.awaitTermination() }}</code></pre></li><li><p>自定义的ValueUtils工具类如下</p><pre><code class="scala">import com.typesafe.config.ConfigFactoryimport org.apache.commons.lang3.StringUtilsobject ValueUtils {val load = ConfigFactory.load() def getStringValue(key:String, defaultValue:String=&quot;&quot;) = {val value = load.getString(key)   if(StringUtils.isNotEmpty(value)) {     value   } else {     defaultValue   } }}</code></pre></li><li><p>application.conf内容如下</p><pre><code class="properties">metadata.broker.list = &quot;192.168.137.251:9092&quot;auto.offset.reset = &quot;smallest&quot;group.id = &quot;hlw_offset_group&quot;kafka.topics = &quot;hlw_offset&quot;serializer.class = &quot;kafka.serializer.StringEncoder&quot;request.required.acks = &quot;1&quot;# JDBC settingsdb.default.driver = &quot;com.mysql.jdbc.Driver&quot;db.default.url=&quot;jdbc:mysql://hadoop000:3306/test&quot;db.default.user=&quot;root&quot;db.default.password=&quot;123456&quot;</code></pre></li><li><p>自定义kafka producer</p><pre><code class="scala">import java.util.{Date, Properties}import kafka.producer.{KeyedMessage, Producer, ProducerConfig}object KafkaProducer { def main(args: Array[String]): Unit = {   val properties = new Properties()   properties.put(&quot;serializer.class&quot;,ValueUtils.getStringValue(&quot;serializer.class&quot;))   properties.put(&quot;metadata.broker.list&quot;,ValueUtils.getStringValue(&quot;metadata.broker.list&quot;))   properties.put(&quot;request.required.acks&quot;,ValueUtils.getStringValue(&quot;request.required.acks&quot;))   val producerConfig = new ProducerConfig(properties)   val producer = new Producer[String,String](producerConfig)   val topic = ValueUtils.getStringValue(&quot;kafka.topics&quot;)   //每次产生100条数据   var i = 0   for (i &lt;- 1 to 100) {     val runtimes = new Date().toString    val messages = new KeyedMessage[String, String](topic,i+&quot;&quot;,&quot;hlw: &quot;+runtimes)     producer.send(messages)   }   println(&quot;数据发送完毕...&quot;) }}</code></pre></li></ol><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ol><li><p>启动kafka服务，并创建主题</p><pre><code class="shell">[hadoop@hadoop000 bin]$ ./kafka-server-start.sh -daemon /home/hadoop/app/kafka_2.11-0.10.0.1/config/server.properties[hadoop@hadoop000 bin]$ ./kafka-topics.sh --list --zookeeper localhost:2181/kafka[hadoop@hadoop000 bin]$ ./kafka-topics.sh --create --zookeeper localhost:2181/kafka --replication-factor 1 --partitions 1 --topic hlw_offset</code></pre></li><li><p>测试前查看MySQL中offset表，刚开始是个空表</p><pre><code class="sql">mysql&gt; select * from hlw_offset;Empty set (0.00 sec)</code></pre></li></ol><ol start="3"><li><p>通过kafka producer产生500条数据</p></li><li><p>启动SparkStreaming程序</p><pre><code class="java">//控制台输出结果：从头开始消费...数据统计记录为：500---hlw_offset,0,0,500---</code></pre></li><li><p>查看MySQL表，offset记录成功</p><pre><code class="sql">mysql&gt; select * from hlw_offset;+------------+------------------+------------+------------+-------------+| topic      | groupid          | partitions | fromoffset | untiloffset |+------------+------------------+------------+------------+-------------+| hlw_offset | hlw_offset_group |          0 |          0 |         500 |+------------+------------------+------------+------------+-------------+</code></pre></li><li><p>关闭SparkStreaming程序，再使用kafka producer生产300条数据,再次启动spark程序（如果spark从500开始消费，说明成功读取了offset，做到了只读取一次语义）</p><pre><code>//控制台结果输出：从已存在记录开始消费...数据统计记录为：300---hlw_offset,0,500,800---</code></pre></li><li><p>查看更新后的offset MySQL数据</p><pre><code class="sql">mysql&gt; select * from hlw_offset;+------------+------------------+------------+------------+-------------+| topic      | groupid          | partitions | fromoffset | untiloffset |+------------+------------------+------------+------------+-------------+| hlw_offset | hlw_offset_group |          0 |        500 |         800 |+------------+------------------+------------+------------+-------------+</code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Spark Streaming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/11/23/hello-world/"/>
      <url>/2019/11/23/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
