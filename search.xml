<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Linux强大的文本分析工具awk详解</title>
      <link href="/2019/11/24/Linux%E5%BC%BA%E5%A4%A7%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7awk%E8%AF%A6%E8%A7%A3/"/>
      <url>/2019/11/24/Linux%E5%BC%BA%E5%A4%A7%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7awk%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="1、awk简介"><a href="#1、awk简介" class="headerlink" title="1、awk简介"></a>1、awk简介</h1><p>awk是一个强大的文本分析工具，也可以说是Linux下一门字符串处理语言，它的数据可以来自标准输入(stdin)、一个文件或多个文件、或其他命令的输出。它可以作为命令使用，但更多的是作为脚本使用。</p><p>它支持正则表达式、支持自定义变量（类map型数组，只不过索引可以是字符串）、支持内置变量、内置函数、及流程控制语句。</p><p>它会依次读取文件的每一行内容, 然后对其进行处理。空格符和制表符为其默认的分割符，使用分割符号对每行进行切片，然后对切开部分进行单独的处理。</p><h1 id="2、awk参数"><a href="#2、awk参数" class="headerlink" title="2、awk参数"></a>2、awk参数</h1><p>支持自定义分隔符</p><p>支持正则表达式匹配 （标准正则）</p><p>支持自定义变量，数组 a[1] a[tom] map(key)</p><p>支持内置变量</p><pre><code class="shell">                   ARGC               命令行参数个数                   ARGV               命令行参数排列                   ENVIRON            支持队列中系统环境变量的使用                   FILENAME           awk浏览的文件名                   FNR                浏览文件的记录数                   FS                 设置输入域分隔符，等价于命令行 -F选项                   NF                 浏览记录的域的个数  本行总列数                   NR                 已读的记录数    行号                   OFS                输出域分隔符                   ORS                输出记录分隔符                   RS                 控制记录分隔符</code></pre><p>控制记录分隔符 支持函数 print、split、substr、sub、gsub</p><p>支持流程控制语句，类C语言 if、while、do/while、for、break、continue</p><p>$0 代表整个记录，$1表示第一个字段，$2表示第二个字段,一次类推。。。。</p><h1 id="3、示例"><a href="#3、示例" class="headerlink" title="3、示例"></a><strong>3、示例</strong></h1><h2 id="文本数据"><a href="#文本数据" class="headerlink" title="文本数据"></a><strong>文本数据</strong></h2><pre><code>root:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologinsync:x:5:0:sync:/sbin:/bin/syncshutdown:x:6:0:shutdown:/sbin:/sbin/shutdownhalt:x:7:0:halt:/sbin:/sbin/haltmail:x:8:12:mail:/var/spool/mail:/sbin/nologinuucp:x:10:14:uucp:/var/spool/uucp:/sbin/nologinoperator:x:11:0:operator:/root:/sbin/nologingames:x:12:100:games:/usr/games:/sbin/nologingopher:x:13:30:gopher:/var/gopher:/sbin/nologinftp:x:14:50:FTP User:/var/ftp:/sbin/nologinnobody:x:99:99:Nobody:/:/sbin/nologinvcsa:x:69:69:virtual console memory owner:/dev:/sbin/nologinsaslauth:x:499:76:&quot;Saslauthd user&quot;:/var/empty/saslauth:/sbin/nologinpostfix:x:89:89::/var/spool/postfix:/sbin/nologinsshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologinmysqladmin:x:514:101::/usr/local/mysql:/bin/bash</code></pre><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><p>按：切割a.txt文件，打印第一列</p><pre><code class="shell">awk -F&#39;:&#39; &#39;{print $1}&#39; a.txt</code></pre><p>按：切割a.txt文件，打印第一列和第七列,tab键分隔</p><pre><code class="shell">awk -F&#39;:&#39; &#39;{print $1 &quot;\t&quot; $7}&#39; a.txt</code></pre><p>按：切割a.txt文件，打印第一列和第七列,表头追加name和shell，表尾追加over，字段间tab键分隔</p><pre><code class="shell">awk -F&#39;:&#39; &#39;BEGIN{print &quot;name\tshell&quot;} {print $1 &quot;\t&quot; $7} END{print &quot;over&quot;}&#39; a.txt</code></pre><p>打印含root单词的行</p><pre><code class="shell">awk &#39;/root/ {print $0} &#39; a.txt</code></pre><p>打印含root单词的行后，再打印所有行（搜索值作用于第一个函数）</p><pre><code class="shell">awk &#39;/root/ {print $0}  {print $0} &#39; a.txt</code></pre><p>打印文件每行的信息，最前方追加行号和该行总列数，用tab键分隔</p><pre><code class="shell">awk -F&#39;:&#39; &#39;{print NR &quot;\t&quot; NF &quot;\t&quot; $0}&#39; a.txt</code></pre><h2 id="报表统计数据"><a href="#报表统计数据" class="headerlink" title="报表统计数据"></a>报表统计数据</h2><pre><code class="shell"># 统计报表：合计每人1月工资，0：manager，1：workerTom      0   2012-12-11      car     3000John     1   2013-01-13      bike    1000vivi     1   2013-01-18      car     2800Tom      0   2013-01-20      car     2500John     1   2013-01-28      bike    3500</code></pre><h3 id="需求-1"><a href="#需求-1" class="headerlink" title="需求"></a>需求</h3><p>统计01月员工工资</p><pre><code class="shell">awk &#39;{split($3,date,&quot;-&quot;);if(date[2]==&quot;01&quot;){name[$1]+=$5}} END{for(i in name){print i &quot;\t&quot; name[i]}}&#39; a.txt</code></pre><p>统计01月员工工资，员工类型为0，末尾追加M，为1末尾追加W</p><pre><code class="shell">awk &#39;{split($3,date,&quot;-&quot;);if(date[2]==&quot;01&quot;){name[$1]+=$5};if($2==&quot;0&quot;){role[$1]=&quot;M&quot;}else{role[$1]=&quot;W&quot;}} END{for(i in name){print i &quot;\t&quot; name[i] &quot;\t&quot; role[i]}}&#39; a.txt</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
            <tag> AWK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark RDD转换成DataFrame的两种方式</title>
      <link href="/2019/11/24/Spark%20RDD%E8%BD%AC%E6%8D%A2%E6%88%90DataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/"/>
      <url>/2019/11/24/Spark%20RDD%E8%BD%AC%E6%8D%A2%E6%88%90DataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>Spark SQL支持两种方式将现有RDD转换为DataFrame。<br>第一种方法使用反射来推断RDD的schema并创建DataSet然后将其转化为DataFrame。这种基于反射方法十分简便，但是前提是在您编写Spark应用程序时就已经知道RDD的schema类型。<br>第二种方法是通过编程接口，使用您构建的StructType，然后将其应用于现有RDD。虽然此方法很麻烦，但它允许您在运行之前并不知道列及其类型的情况下构建DataSet<br>        方法如下：<br>            1.将RDD转换成Rows<br>            2.按照第一步Rows的结构定义StructType<br>            3.基于rows和StructType使用createDataFrame创建相应的DF</p><p><strong>测试数据为order.data</strong></p><pre><code>1   小王  电视  12  2015-08-01 09:08:311   小王  冰箱  24  2015-08-01 09:08:142   小李  空调  12  2015-09-02 09:01:31</code></pre><p><strong>代码如下:</strong></p><pre><code class="scala">object RDD2DF {  /**    * 主要有两种方式    *   第一种是在已经知道schema已经知道的情况下，我们使用反射把RDD转换成DS，进而转换成DF    *   第二种是你不能提前定义好case class，例如数据的结构是以String类型存在的。我们使用接口自定义一个schema    * @param args    */  def main(args: Array[String]): Unit = {    val spark=SparkSession.builder()      .appName(&quot;DFDemo&quot;)      .master(&quot;local[2]&quot;)      .getOrCreate()//    rdd2DFFunc1(spark)    rdd2DFFunc2(spark)    spark.stop()  }  /**    * 提前定义好case class    * @param spark    */  def rdd2DFFunc1(spark:SparkSession): Unit ={    import spark.implicits._    val orderRDD=spark.sparkContext.textFile(&quot;F:\\JAVA\\WorkSpace\\spark\\src\\main\\resources\\order.data&quot;)    val orderDF=orderRDD.map(_.split(&quot;\t&quot;))      .map(attributes=&gt;Order(attributes(0),attributes(1),attributes(2),attributes(3),attributes(4)))      .toDF()    orderDF.show()    Thread.sleep(1000000)  }  /**    *总结：第二种方式就是通过最基础的DF接口方法，将    * @param spark    */  def rdd2DFFunc2(spark:SparkSession): Unit ={    //TODO:   1.将RDD转换成Rows   2.按照第一步Rows的结构定义StructType  3.基于rows和StructType使用createDataFrame创建相应的DF    val orderRDD=spark.sparkContext.textFile(&quot;F:\\JAVA\\WorkSpace\\spark\\src\\main\\resources\\order.data&quot;)    //TODO:   1.将RDD转换成Rows    val rowsRDD=orderRDD//      .filter((str:String)=&gt;{val arr=str.split(&quot;\t&quot;);val res=arr(1)!=&quot;小李&quot;;res})      .map(_.split(&quot;\t&quot;))      .map(attributes=&gt;Row(attributes(0).trim,attributes(1),attributes(2),attributes(3).trim,attributes(4)))    //TODO:   2.按照第一步Rows的结构定义StructType    val schemaString=&quot;id|name|commodity|age|date&quot;    val fields=schemaString.split(&quot;\\|&quot;)      .map(filedName=&gt;StructField(filedName,StringType,nullable = true))    val schema=StructType(fields)    //TODO:   3.基于rows和StructType使用createDataFrame创建相应的DF   val orderDF= spark.createDataFrame(rowsRDD,schema)    orderDF.show()    orderDF.groupBy(&quot;name&quot;).count().show()    orderDF.select(&quot;name&quot;,&quot;commodity&quot;).show()    Thread.sleep(10000000)  }}case class Order(id:String,name:String,commodity:String,age:String,date:String)</code></pre><h1 id="生产中创建DataFrame代码举例"><a href="#生产中创建DataFrame代码举例" class="headerlink" title="生产中创建DataFrame代码举例"></a>生产中创建DataFrame代码举例</h1><p>在实际生产环境中，我们其实选择的是方式二这种进行创建DataFrame的，因为我们生产中很难提前定义case class ，因为业务处理之后字段常常会发生意想不到的变化，所以一定要掌握这种方法。</p><h2 id="测试数据"><a href="#测试数据" class="headerlink" title="测试数据"></a>测试数据</h2><pre><code>baidu   CN  A   E   [01/May/2018:02:15:52 +0800]    2   61.237.59.0 -   112.29.213.35:80    0   movieshow2000.edu.chinaren.com  GET http://movieshow2000.edu.chinaren.com/user_upload/15316339776271455.mp4 HTTP/1.1    -   bytes 13869056-13885439/25136186    TCP_HIT/206 112.29.213.35   video/mp4   16374   16384   -:0 0   0   -   -   -   11451601    -   &quot;JSP3/2.0.14&quot;   &quot;-&quot; &quot;-&quot; &quot;-&quot; http    -   2   v1.go2yd.com    0.002   25136186    16384   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   1531818470104-11451601-112.29.213.66#2705261172 644514568baidu   CN  A   E   [01/May/2018:02:25:33 +0800]    2   61.232.37.228   -   112.29.213.35:80    0   github.com  GET http://github.com/user_upload/15316339776271/44y.mp4    HTTP/1.1    -   bytes 13869056-13885439/25136186    TCP_HIT/206 112.29.213.35   video/mp4   83552   16384   -:0 0   0   -   -   -   11451601    -   &quot;JSP3/2.0.14&quot;   &quot;-&quot; &quot;-&quot; &quot;-&quot; http    -   2   v1.go2yd.com    0.002   25136186    16384   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   1531818470104-11451601-112.29.213.66#2705261172 644514568</code></pre><h2 id="Schema方法类"><a href="#Schema方法类" class="headerlink" title="Schema方法类"></a>Schema方法类</h2><pre><code class="scala">import org.apache.spark.sql.Rowimport org.apache.spark.sql.types.{LongType, StringType, StructField, StructType}object LogConverUtil {  private val struct=StructType(    Array(      StructField(&quot;domain&quot;,StringType)      ,StructField(&quot;url&quot;,StringType)      ,StructField(&quot;pv&quot;,LongType)      ,StructField(&quot;traffic&quot;,LongType)      ,StructField(&quot;date&quot;,StringType)    )  )  def getStruct():StructType={    struct  }  def parseLog(logLine:String): Row ={    val sourceFormat=new SimpleDateFormat(&quot;[dd/MMM/yyyy:hh:mm:ss +0800]&quot;,Locale.ENGLISH)    val targetFormat=new SimpleDateFormat(&quot;yyyyMMddhh&quot;)    try{      val fields=logLine.split(&quot;\t&quot;)      val domain=fields(10)      val url=fields(12)      val pv=1L      val traffic=fields(19).trim.toLong      val date=getFormatedDate(fields(4),sourceFormat,targetFormat)      Row(domain,url,pv,traffic,date)    }catch {      case e:Exception=&gt;Row(0)    }  }  /**    *    * @param sourceDate  Log中的未格式化日期   [01/May/2018:01:09:45 +0800]    * @return  按照需求格式化字段      2018050101    */  def getFormatedDate(sourceDate: String, sourceFormat: SimpleDateFormat, targetFormat: SimpleDateFormat) = {    val targetTime=targetFormat.format(sourceFormat.parse(sourceDate))    targetTime  }}</code></pre><h2 id="RDD2DataFrame主类"><a href="#RDD2DataFrame主类" class="headerlink" title="RDD2DataFrame主类"></a>RDD2DataFrame主类</h2><pre><code class="scala">import org.apache.spark.sql.SparkSessionobject SparkCleanJob {  def main(args: Array[String]): Unit = {    val spark=SparkSession.builder()      .master(&quot;local[2]&quot;)      .appName(&quot;SparkCleanJob&quot;)      .getOrCreate()    val logRDD=spark.sparkContext.textFile(&quot;file:///D:/baidu.log&quot;)//    logRDD.take(2).foreach(println(_))    //调用LogConverUtil里的parseLog方法和getStruct方法获得Rows对象和StructType对象    val logDF=spark.createDataFrame(logRDD.map(LogConverUtil.parseLog(_)),LogConverUtil.getStruct())    logDF.show(false)    logDF.printSchema()  }}</code></pre><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><pre><code class="sql">+------------------------------+-------------------------------------------------------------------------+---+-------+----------+|domain                        |url                                                                      |pv |traffic|date      |+------------------------------+-------------------------------------------------------------------------+---+-------+----------+|movieshow2000.edu.chinaren.com|http://movieshow2000.edu.chinaren.com/user_upload/15316339776271455.mp4  |1  |16374  |2018050102||github.com                    |http://github.com/user_upload/15316339776271/44y.mp4                     |1  |83552  |2018050102||yooku.com                     |http://yooku.com/user_upload/15316339776271x0.html                       |1  |74986  |2018050101||rw.uestc.edu.cn               |http://rw.uestc.edu.cn/user_upload/15316339776271515.mp4                 |1  |55297  |2018050101||github.com                    |http://github.com/user_upload/15316339776271x05.mp4                      |1  |26812  |2018050102||movieshow2000.edu.chinaren.com|http://movieshow2000.edu.chinaren.com/user_upload/15316339776271y4.html  |1  |50392  |2018050103||github.com                    |http://github.com/user_upload/15316339776271x15.html                     |1  |40092  |2018050101||movieshow2000.edu.chinaren.com|http://movieshow2000.edu.chinaren.com/user_upload/153163397762714z.mp4   |1  |8368   |2018050102||movieshow2000.edu.chinaren.com|http://movieshow2000.edu.chinaren.com/user_upload/15316339776271/5z.html |1  |29677  |2018050103||rw.uestc.edu.cn               |http://rw.uestc.edu.cn/user_upload/153163397762710w.mp4                  |1  |26124  |2018050102||yooku.com                     |http://yooku.com/user_upload/15316339776271yz.mp4                        |1  |32219  |2018050101||yooku.com                     |http://yooku.com/user_upload/153163397762713w.html                       |1  |90389  |2018050101||movieshow2000.edu.chinaren.com|http://movieshow2000.edu.chinaren.com/user_upload/15316339776271z/.html  |1  |15623  |2018050101||yooku.com                     |http://yooku.com/user_upload/1531633977627142.html                       |1  |53453  |2018050103||yooku.com                     |http://yooku.com/user_upload/15316339776271230.mp4                       |1  |20309  |2018050102||movieshow2000.edu.chinaren.com|http://movieshow2000.edu.chinaren.com/user_upload/15316339776271/4w1.html|1  |87804  |2018050103||movieshow2000.edu.chinaren.com|http://movieshow2000.edu.chinaren.com/user_upload/15316339776271y5y.html |1  |69469  |2018050103||yooku.com                     |http://yooku.com/user_upload/15316339776271011/.mp4                      |1  |3782   |2018050103||github.com                    |http://github.com/user_upload/15316339776271wzw.mp4                      |1  |89642  |2018050102||github.com                    |http://github.com/user_upload/15316339776271/1/.mp4                      |1  |63551  |2018050103|+------------------------------+-------------------------------------------------------------------------+---+-------+----------+only showing top 20 rowsroot |-- domain: string (nullable = true) |-- url: string (nullable = true) |-- pv: long (nullable = true) |-- traffic: long (nullable = true) |-- date: string (nullable = true)Process finished with exit code 0</code></pre><p><strong>注:除了这种使用RDD读取文本进而转化成DataFrame之外，我们也会使用自定义DefaultSource来直接将text转化成DataFrame</strong></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark core </tag>
            
            <tag> Spark SQL </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive底层执行流程</title>
      <link href="/2019/11/24/Hive%E5%BA%95%E5%B1%82%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/"/>
      <url>/2019/11/24/Hive%E5%BA%95%E5%B1%82%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>Hive并不是简简单单写SQL，因为我们要进行层层调优，如果连Hive的内部运行机制都搞不清，那么hive对我们来说仅仅是一个黑箱，高效率的调优无从谈起，所以我们很有必要了解下Hive是如何将SQL转化为MapReduce任务的呢？</p><h1 id="Hive-底层执行流程"><a href="#Hive-底层执行流程" class="headerlink" title="Hive 底层执行流程"></a>Hive 底层执行流程</h1><p>我们以下面这个SQL为例</p><pre><code class="SQL">FROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,4)) GROUP BY src.key;</code></pre><p>整个编译过程分为六个阶段：<br><strong>1.Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象语法树AST Tree</strong><br><a href="https://github.com/apache/hive/blob/branch-1.1/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g" target="_blank" rel="noopener">HiveLexerX</a>，<a href="https://github.com/apache/hive/blob/branch-1.1/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g" target="_blank" rel="noopener">HiveParser</a>分别是Antlr对SQL编译后自动生成的词法解析和语法解析类，在这两个类中进行复杂的解析。<br>例子中的AST tree为</p><pre><code class="java">ABSTRACT SYNTAX TREE:(TOK_QUERY (TOK_FROM (TOK_TABREF src))(TOK_INSERT (TOK_DESTINATION (TOK_TAB dest_g1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src key)) (TOK_SELEXPR (TOK_FUNCTION sum (TOK_FUNCTION substr (TOK_COLREF src value) 4))))(TOK_GROUPBY (TOK_COLREF src key))))</code></pre><p><strong>2.遍历AST Tree，抽象出查询的基本组成单元QueryBlock</strong><br>AST Tree 仍然非常复杂，不够结构化，不方便直接翻译为 MapReduce 程序， AST<br>Tree 转化为 <a href="https://github.com/apache/hive/blob/branch-1.1/ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java" target="_blank" rel="noopener">QueryBlock</a>(QB)就是将 SQL 进一部抽象和结构化。<br>AST Tree 生成 QueryBlock 的过程是一个递归的过程，先序遍历 AST Tree ，遇到不<br>同的Token 节点(理解为特殊标记)，保存到相应的属性中，主要包含以下几个过程</p><pre><code class="java">TOK_QUERY =&gt; 创建 QB 对象，循环递归子节点TOK_FROM =&gt; 将表名语法部分保存到 QB 对象的 aliasToTabs 等属性中TOK_INSERT =&gt; 循环递归子节点TOK_DESTINATION =&gt; 将输出目标的语法部分保存在 QBParseInfo 对象的nameToDest 属性中TOK_SELECT =&gt; 分别将查询表达式的语法部分保存在 destToSelExpr 、destToAggregationExprs 、 destToDistinctFuncExprs 三个属性中TOK_WHERE =&gt; 将 Where 部分的语法保存在 QBParseInfo 对象的destToWhereExpr 属性中 </code></pre><p><strong>3.遍历QueryBlock，翻译为执行操作树OperatorTree</strong><br>Hive 最终生成的 MapReduce 任务， Map 阶段和 Reduce 阶段均由 Operator Tree<br>组成。逻辑操作符，就是在 Map 阶段或者 Reduce 阶段完成单一特定的操作。<br>基本的操作符包括<br>TableScanOperator、SelectOperator、FilterOperator、JoinOperator、GroupByOperator、ReduceSinkOperator<br>QueryBlock 生成 Operator Tree 就是遍历上一个过程中生成的 QB 和 QBParseInfo<br>对象的保存<br>语法的属性，包含如下几个步骤：</p><pre><code>QB#aliasToSubq =&gt; 有子查询，递归调用QB#aliasToTabs =&gt; TableScanOperatorQBParseInfo#joinExpr =&gt; QBJoinTree =&gt; ReduceSinkOperator + JoinOperatorQBParseInfo#destToWhereExpr =&gt; FilterOperatorQBParseInfo#destToGroupby =&gt; ReduceSinkOperator +GroupByOperatorQBParseInfo#destToOrderby =&gt; ReduceSinkOperator + ExtractOperator</code></pre><p>由于 Join/GroupBy/OrderBy 均需要在 Reduce 阶段完成，所以在生成相应操作的Operator 之前都会先生成一个 ReduceSinkOperator ，将字段组合并序列化为 Reduce Key/value,Partition Key<br><strong>SQL例子翻译成OperatorTree</strong></p><pre><code class="sql">STAGE PLANS:  Stage: Stage-1    Map Reduce      Alias -&gt; Map Operator Tree:        src            Reduce Output Operator              key expressions:                    expr: key                    type: string              sort order: +              Map-reduce partition columns:                    expr: rand()                    type: double              tag: -1              value expressions:                    expr: substr(value, 4)                    type: string      Reduce Operator Tree:        Group By Operator          aggregations:                expr: sum(UDFToDouble(VALUE.0))          keys:                expr: KEY.0                type: string          mode: partial1          File Output Operator            compressed: false            table:                input format: org.apache.hadoop.mapred.SequenceFileInputFormat                output format: org.apache.hadoop.mapred.SequenceFileOutputFormat                name: binary_table  Stage: Stage-2    Map Reduce      Alias -&gt; Map Operator Tree:        /tmp/hive-zshao/67494501/106593589.10001          Reduce Output Operator            key expressions:                  expr: 0                  type: string            sort order: +            Map-reduce partition columns:                  expr: 0                  type: string            tag: -1            value expressions:                  expr: 1                  type: double      Reduce Operator Tree:        Group By Operator          aggregations:                expr: sum(VALUE.0)          keys:                expr: KEY.0                type: string          mode: final          Select Operator            expressions:                  expr: 0                  type: string                  expr: 1                  type: double            Select Operator              expressions:                    expr: UDFToInteger(0)                    type: int                    expr: 1                    type: double              File Output Operator                compressed: false                table:                    input format: org.apache.hadoop.mapred.TextInputFormat                    output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat                    serde: org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe                    name: dest_g1  Stage: Stage-0    Move Operator      tables:            replace: true            table:                input format: org.apache.hadoop.mapred.TextInputFormat                output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat                serde: org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe                name: dest_g1</code></pre><p><strong>4.Logical Optimizer进行OperatorTree变换，合并不必要的</strong><br>使用<a href="https://github.com/apache/hive/blob/branch-1.1/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java" target="_blank" rel="noopener">ReduceSinkOperator</a>，减少shuffle数据量。大部分逻辑层优化器通过变换 OperatorTree ，合并操作符，达到减少 MapReduce Job ，减少 shuffle 数据量的目的。<br><strong>5.遍历OperatorTree，翻译为Task tree</strong><br>OperatorTree 转化为 Task tree的过程分为下面几个阶段</p><ul><li>对输出表生成 MoveTask</li><li>从 OperatorTree 的其中一个根节点向下深度优先遍历</li><li>ReduceSinkOperator 标示 Map/Reduce 的界限，多个 Job 间的界限</li><li>遍历其他根节点，遇过碰到 JoinOperator 合并 MapReduceTask</li><li>生成 StatTask 更新元数据</li><li>剪断 Map 与 Reduce 间的 Operator 的关系</li></ul><p><strong>6.PhysicalOptimizer 对Task tree优化，生成最终的执行计划</strong></p><p><strong>7、执行</strong></p><p>以上就是HiveSQL的底层执行流程</p><h1 id="打印SQL运行相关信息"><a href="#打印SQL运行相关信息" class="headerlink" title="打印SQL运行相关信息"></a>打印SQL运行相关信息</h1><p>我们在开发中，可以使用下面这个语句来打印SQL语句的相关运行信息</p><pre><code class="sql">EXPLAIN [EXTENDED|DEPENDENCY|AUTHORIZATION] query</code></pre><p>注:我的版本是<strong>hive-1.1.0-cdh5.7.0</strong>，所以只可用三个可选属性，如果您版本比较高的话，可以去<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Explain" target="_blank" rel="noopener">官网</a>查阅对应属性<br>下面我对三种可选属性进行简单介绍</p><h2 id="EXTENDED"><a href="#EXTENDED" class="headerlink" title="EXTENDED"></a>EXTENDED</h2><p>EXTENDED：打印SQL解析成AST&amp;Operator Tree最全面的信息</p><pre><code class="sql">hive (g6_hadoop)&gt; explain EXTENDED insert  OVERWRITE table g6_access_orc_explain select domain,count(1) num from g6_access_orc where traffic&gt;&#39;99900&#39; group by domain;OKExplainABSTRACT SYNTAX TREE:TOK_QUERY   TOK_FROM      TOK_TABREF         TOK_TABNAME            g6_access_orc   TOK_INSERT      TOK_DESTINATION         TOK_TAB            TOK_TABNAME               g6_access_orc_explain      TOK_SELECT         TOK_SELEXPR            TOK_TABLE_OR_COL               domain         TOK_SELEXPR            TOK_FUNCTION               count               1            num      TOK_WHERE         &gt;            TOK_TABLE_OR_COL               traffic            &#39;99900&#39;      TOK_GROUPBY         TOK_TABLE_OR_COL            domainSTAGE DEPENDENCIES:  Stage-1 is a root stage  Stage-0 depends on stages: Stage-1  Stage-2 depends on stages: Stage-0STAGE PLANS:  Stage: Stage-1    Map Reduce      Map Operator Tree:          TableScan            alias: g6_access_orc            Statistics: Num rows: 260326 Data size: 188215698 Basic stats: COMPLETE Column stats: NONE            GatherStats: false            Filter Operator              isSamplingPred: false              predicate: (traffic &gt; 99900) (type: boolean)              Statistics: Num rows: 86775 Data size: 62738325 Basic stats: COMPLETE Column stats: NONE              Select Operator                expressions: domain (type: string)                outputColumnNames: domain                Statistics: Num rows: 86775 Data size: 62738325 Basic stats: COMPLETE Column stats: NONE                Group By Operator                  aggregations: count(1)                  keys: domain (type: string)                  mode: hash                  outputColumnNames: _col0, _col1                  Statistics: Num rows: 86775 Data size: 62738325 Basic stats: COMPLETE Column stats: NONE                  Reduce Output Operator                    key expressions: _col0 (type: string)                    sort order: +                    Map-reduce partition columns: _col0 (type: string)                    Statistics: Num rows: 86775 Data size: 62738325 Basic stats: COMPLETE Column stats: NONE                    tag: -1                    value expressions: _col1 (type: bigint)                    auto parallelism: false      Path -&gt; Alias:        hdfs://ruozeclusterg6/user/hive/warehouse/g6_hadoop.db/g6_access_orc [g6_access_orc]      Path -&gt; Partition:        hdfs://ruozeclusterg6/user/hive/warehouse/g6_hadoop.db/g6_access_orc           Partition            base file name: g6_access_orc            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat            properties:              COLUMN_STATS_ACCURATE true              bucket_count -1              columns cdn,region,level,time,ip,domain,url,traffic              columns.comments               columns.types string:string:string:string:string:string:string:bigint              field.delim                 file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat              location hdfs://ruozeclusterg6/user/hive/warehouse/g6_hadoop.db/g6_access_orc              name g6_hadoop.g6_access_orc              numFiles 1              numRows 260326              rawDataSize 188215698              serialization.ddl struct g6_access_orc { string cdn, string region, string level, string time, string ip, string domain, string url, i64 traffic}              serialization.format                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde              totalSize 8567798              transient_lastDdlTime 1557676635            serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat              properties:                COLUMN_STATS_ACCURATE true                bucket_count -1                columns cdn,region,level,time,ip,domain,url,traffic                columns.comments                 columns.types string:string:string:string:string:string:string:bigint                field.delim                     file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat                location hdfs://ruozeclusterg6/user/hive/warehouse/g6_hadoop.db/g6_access_orc                name g6_hadoop.g6_access_orc                numFiles 1                numRows 260326                rawDataSize 188215698                serialization.ddl struct g6_access_orc { string cdn, string region, string level, string time, string ip, string domain, string url, i64 traffic}                serialization.format                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde                totalSize 8567798                transient_lastDdlTime 1557676635              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde              name: g6_hadoop.g6_access_orc            name: g6_hadoop.g6_access_orc      Truncated Path -&gt; Alias:        /g6_hadoop.db/g6_access_orc [g6_access_orc]      Needs Tagging: false      Reduce Operator Tree:        Group By Operator          aggregations: count(VALUE._col0)          keys: KEY._col0 (type: string)          mode: mergepartial          outputColumnNames: _col0, _col1          Statistics: Num rows: 43387 Data size: 31368801 Basic stats: COMPLETE Column stats: NONE          File Output Operator            compressed: false            GlobalTableId: 1            directory: hdfs://ruozeclusterg6/user/hive/warehouse/g6_hadoop.db/g6_access_orc_explain/.hive-staging_hive_2019-05-23_23-37-06_889_2210604962026719569-1/-ext-10000            NumFilesPerFileSink: 1            Statistics: Num rows: 43387 Data size: 31368801 Basic stats: COMPLETE Column stats: NONE            Stats Publishing Key Prefix: hdfs://ruozeclusterg6/user/hive/warehouse/g6_hadoop.db/g6_access_orc_explain/.hive-staging_hive_2019-05-23_23-37-06_889_2210604962026719569-1/-ext-10000/            table:                input format: org.apache.hadoop.mapred.TextInputFormat                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat                properties:                  COLUMN_STATS_ACCURATE true                  bucket_count -1                  columns domain,num                  columns.comments                   columns.types string:bigint                  field.delim |                  file.inputformat org.apache.hadoop.mapred.TextInputFormat                  file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat                  location hdfs://ruozeclusterg6/user/hive/warehouse/g6_hadoop.db/g6_access_orc_explain                  name g6_hadoop.g6_access_orc_explain                  numFiles 1                  numRows 7                  rawDataSize 149                  serialization.ddl struct g6_access_orc_explain { string domain, i64 num}                  serialization.format |                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                  totalSize 156                  transient_lastDdlTime 1558661108                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                name: g6_hadoop.g6_access_orc_explain            TotalFiles: 1            GatherStats: true            MultiFileSpray: false  Stage: Stage-0    Move Operator      tables:          replace: true          source: hdfs://ruozeclusterg6/user/hive/warehouse/g6_hadoop.db/g6_access_orc_explain/.hive-staging_hive_2019-05-23_23-37-06_889_2210604962026719569-1/-ext-10000          table:              input format: org.apache.hadoop.mapred.TextInputFormat              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat              properties:                COLUMN_STATS_ACCURATE true                bucket_count -1                columns domain,num                columns.comments                 columns.types string:bigint                field.delim |                file.inputformat org.apache.hadoop.mapred.TextInputFormat                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat                location hdfs://ruozeclusterg6/user/hive/warehouse/g6_hadoop.db/g6_access_orc_explain                name g6_hadoop.g6_access_orc_explain                numFiles 1                numRows 7                rawDataSize 149                serialization.ddl struct g6_access_orc_explain { string domain, i64 num}                serialization.format |                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                totalSize 156                transient_lastDdlTime 1558661108              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe              name: g6_hadoop.g6_access_orc_explain  Stage: Stage-2    Stats-Aggr Operator      Stats Aggregation Key Prefix: hdfs://ruozeclusterg6/user/hive/warehouse/g6_hadoop.db/g6_access_orc_explain/.hive-staging_hive_2019-05-23_23-37-06_889_2210604962026719569-1/-ext-10000/Time taken: 1.359 seconds, Fetched: 198 row(s)</code></pre><h2 id="AUTHORIZATION"><a href="#AUTHORIZATION" class="headerlink" title="AUTHORIZATION"></a>AUTHORIZATION</h2><p>AUTHORIZATION :打印SQL运行相关权限</p><pre><code class="sql">hive (g6_hadoop)&gt; explain AUTHORIZATION insert  OVERWRITE table g6_access_orc_explain select domain,count(1) num from g6_access_orc where traffic&gt;&#39;99900&#39; group by domain;OKExplainINPUTS:   g6_hadoop@g6_access_orcOUTPUTS:   g6_hadoop@g6_access_orc_explainCURRENT_USER:   hadoopOPERATION:   QUERYAUTHORIZATION_FAILURES:   No privilege &#39;Update&#39; found for outputs { database:g6_hadoop, table:g6_access_orc_explain}  No privilege &#39;Select&#39; found for inputs { database:g6_hadoop, table:g6_access_orc, columnName:domain}Time taken: 0.599 seconds, Fetched: 11 row(s)</code></pre><h2 id="DEPENDENCY"><a href="#DEPENDENCY" class="headerlink" title="DEPENDENCY"></a>DEPENDENCY</h2><p>DEPENDENCY:打印SQL输入表的相关信息</p><pre><code class="sql">hive (g6_hadoop)&gt; explain DEPENDENCY insert  OVERWRITE table g6_access_orc_explain select domain,count(1) num from g6_access_orc where traffic&gt;&#39;99900&#39; group by domain;Explain{&quot;input_partitions&quot;:[],&quot;input_tables&quot;:[{&quot;tablename&quot;:&quot;g6_hadoop@g6_access_orc&quot;,&quot;tabletype&quot;:&quot;MANAGED_TABLE&quot;}]}Time taken: 0.135 seconds, Fetched: 1 row(s)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Maxwell读取MySQL binlog日志到Kafka</title>
      <link href="/2019/11/23/Maxwell%E8%AF%BB%E5%8F%96MySQL%E3%81%AEbinlog%E6%97%A5%E5%BF%97%E5%88%B0Kafka/"/>
      <url>/2019/11/23/Maxwell%E8%AF%BB%E5%8F%96MySQL%E3%81%AEbinlog%E6%97%A5%E5%BF%97%E5%88%B0Kafka/</url>
      
        <content type="html"><![CDATA[<h3 id="启动MySQL"><a href="#启动MySQL" class="headerlink" title="启动MySQL"></a>启动MySQL</h3><h3 id="创建maxwell的数据库和用户"><a href="#创建maxwell的数据库和用户" class="headerlink" title="创建maxwell的数据库和用户"></a>创建maxwell的数据库和用户</h3><h3 id="在MySQL中创建一个测试数据库和表"><a href="#在MySQL中创建一个测试数据库和表" class="headerlink" title="在MySQL中创建一个测试数据库和表"></a>在MySQL中创建一个测试数据库和表</h3><p>前面三个步骤详见 <a href="https://blog.51cto.com/14309075/2415503" target="_blank" rel="noopener">Maxwell读取MySQL binlog日志通过stdout展示</a></p><h3 id="启动Zookeeper"><a href="#启动Zookeeper" class="headerlink" title="启动Zookeeper"></a>启动Zookeeper</h3><pre><code class="shell">[hadoop@hadoop001 ~]$ cd $ZK_HOME/bin[hadoop@hadoop001 bin]$ ./zkServer.sh start</code></pre><h3 id="启动kafka，并创建主题为maxwell的topic"><a href="#启动kafka，并创建主题为maxwell的topic" class="headerlink" title="启动kafka，并创建主题为maxwell的topic"></a>启动kafka，并创建主题为maxwell的topic</h3><pre><code class="shell">[hadoop@hadoop001 bin]$ cd $KAFKA_HOME//查看kafka版本，防止maxwell不支持[hadoop@hadoop001 kafka]$ find ./libs/ -name \*kafka_\* | head -1 | grep -o &#39;\kafka[^\n]*&#39;kafka_2.11-0.10.0.1-sources.jar//启动kafka-server服务[hadoop@hadoop001 kafka]$ nohup bin/kafka-server-start.sh config/server.properties &amp;[hadoop@hadoop001 kafka]$ jps13460 QuorumPeerMain14952 Jps13518 Kafka[hadoop@hadoop001 kafka]$ bin/kafka-topics.sh --create --zookeeper 192.168.137.2:2181/kafka --replication-factor 1 --partitions 3 --topic maxwellCreated topic &quot;maxwell&quot;.[hadoop@hadoop001 kafka]$ bin/kafka-topics.sh --list --zookeeper 192.168.137.2:2181/kafka__consumer_offsetsmaxwell</code></pre><h3 id="启动kafaka的消费者，检查数据是否到位"><a href="#启动kafaka的消费者，检查数据是否到位" class="headerlink" title="启动kafaka的消费者，检查数据是否到位"></a>启动kafaka的消费者，检查数据是否到位</h3><pre><code class="shell">[hadoop@hadoop001 kafka]$ bin/kafka-console-consumer.sh --zookeeper 192.168.137.2:2181/kafka --topic maxwell --from-beginning</code></pre><h3 id="启动maxwell进程"><a href="#启动maxwell进程" class="headerlink" title="启动maxwell进程"></a>启动maxwell进程</h3><pre><code class="shell">//先检查maxwell是否支持kafka-0.10.0.1[root@hadoop000 ~]# cd /root/app/maxwell-1.17.1/lib/kafka-clients[root@hadoop001 kafka-clients]# lltotal 5556-rw-r--r-- 1 yarn games  746207 Jul  3  2018 kafka-clients-0.10.0.1.jar-rw-r--r-- 1 yarn games  951041 Jul  3  2018 kafka-clients-0.10.2.1.jar-rw-r--r-- 1 yarn games 1419544 Jul  3  2018 kafka-clients-0.11.0.1.jar-rw-r--r-- 1 yarn games  324016 Jul  3  2018 kafka-clients-0.8.2.2.jar-rw-r--r-- 1 yarn games  641408 Jul  3  2018 kafka-clients-0.9.0.1.jar-rw-r--r-- 1 yarn games 1591338 Jul  3  2018 kafka-clients-1.0.0.jar//发现支持kafka-0.10.0.1版本，假如没有生产上正在用的kafka版本的jar包，可以直接把这个版本的client jar包copy进来//启动maxwell[root@hadoop001 maxwell-1.17.1]# bin/maxwell --user=&#39;maxwell&#39; --password=&#39;maxwell&#39; --host=&#39;127.0.0.1&#39; --producer=kafka --kafka_version=0.10.0.1 --kafka.bootstrap.servers=192.168.137.2:9092 --kafka_topic=maxwellUsing kafka version: 0.10.0.110:16:52,979 WARN  MaxwellMetrics - Metrics will not be exposed: metricsReportingType not configured.10:16:53,451 INFO  ProducerConfig - ProducerConfig values:         metric.reporters = []        metadata.max.age.ms = 300000        reconnect.backoff.ms = 50        sasl.kerberos.ticket.renew.window.factor = 0.8        bootstrap.servers = [192.168.137.2:9092]        ssl.keystore.type = JKS        sasl.mechanism = GSSAPI        max.block.ms = 60000        interceptor.classes = null        ssl.truststore.password = null        client.id =         ssl.endpoint.identification.algorithm = null        request.timeout.ms = 30000        acks = 1        receive.buffer.bytes = 32768        ssl.truststore.type = JKS        retries = 0        ssl.truststore.location = null        ssl.keystore.password = null        send.buffer.bytes = 131072        compression.type = none        metadata.fetch.timeout.ms = 60000        retry.backoff.ms = 100        sasl.kerberos.kinit.cmd = /usr/bin/kinit        buffer.memory = 33554432        timeout.ms = 30000        key.serializer = class org.apache.kafka.common.serialization.StringSerializer        sasl.kerberos.service.name = null        sasl.kerberos.ticket.renew.jitter = 0.05        ssl.trustmanager.algorithm = PKIX        block.on.buffer.full = false        ssl.key.password = null        sasl.kerberos.min.time.before.relogin = 60000        connections.max.idle.ms = 540000        max.in.flight.requests.per.connection = 5        metrics.num.samples = 2        ssl.protocol = TLS        ssl.provider = null        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]        batch.size = 16384        ssl.keystore.location = null        ssl.cipher.suites = null        security.protocol = PLAINTEXT        max.request.size = 1048576        value.serializer = class org.apache.kafka.common.serialization.StringSerializer        ssl.keymanager.algorithm = SunX509        metrics.sample.window.ms = 30000        partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner        linger.ms = 010:16:53,512 INFO  ProducerConfig - ProducerConfig values:         metric.reporters = []        metadata.max.age.ms = 300000        reconnect.backoff.ms = 50        sasl.kerberos.ticket.renew.window.factor = 0.8        bootstrap.servers = [192.168.137.2:9092]        ssl.keystore.type = JKS        sasl.mechanism = GSSAPI        max.block.ms = 60000        interceptor.classes = null        ssl.truststore.password = null        client.id = producer-1        ssl.endpoint.identification.algorithm = null        request.timeout.ms = 30000        acks = 1        receive.buffer.bytes = 32768        ssl.truststore.type = JKS        retries = 0        ssl.truststore.location = null        ssl.keystore.password = null        send.buffer.bytes = 131072        compression.type = none        metadata.fetch.timeout.ms = 60000        retry.backoff.ms = 100        sasl.kerberos.kinit.cmd = /usr/bin/kinit        buffer.memory = 33554432        timeout.ms = 30000        key.serializer = class org.apache.kafka.common.serialization.StringSerializer        sasl.kerberos.service.name = null        sasl.kerberos.ticket.renew.jitter = 0.05        ssl.trustmanager.algorithm = PKIX        block.on.buffer.full = false        ssl.key.password = null        sasl.kerberos.min.time.before.relogin = 60000        connections.max.idle.ms = 540000        max.in.flight.requests.per.connection = 5        metrics.num.samples = 2        ssl.protocol = TLS        ssl.provider = null        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]        batch.size = 16384        ssl.keystore.location = null        ssl.cipher.suites = null        security.protocol = PLAINTEXT        max.request.size = 1048576        value.serializer = class org.apache.kafka.common.serialization.StringSerializer        ssl.keymanager.algorithm = SunX509        metrics.sample.window.ms = 30000        partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner        linger.ms = 010:16:53,516 INFO  AppInfoParser - Kafka version : 0.10.0.110:16:53,516 INFO  AppInfoParser - Kafka commitId : a7a17cdec9eaa6c510:16:53,550 INFO  Maxwell - Maxwell v1.17.1 is booting (MaxwellKafkaProducer), starting at Position[BinlogPosition[mysql-bin.000016:116360], lastHeartbeat=1552092988288]10:16:53,730 INFO  MysqlSavedSchema - Restoring schema id 1 (last modified at Position[BinlogPosition[mysql-bin.000014:5999], lastHeartbeat=0])10:16:53,846 INFO  BinlogConnectorReplicator - Setting initial binlog pos to: mysql-bin.000016:11636010:16:53,951 INFO  BinaryLogClient - Connected to 127.0.0.1:3306 at mysql-bin.000016/116360 (sid:6379, cid:4)10:16:53,951 INFO  BinlogConnectorLifecycleListener - Binlog connected.</code></pre><h3 id="在MySQL中更新一条数据"><a href="#在MySQL中更新一条数据" class="headerlink" title="在MySQL中更新一条数据"></a>在MySQL中更新一条数据</h3><pre><code class="sql">mysql&gt; update emp set sal=502 where empno=6001;mysql&gt; update emp set sal=603 where empno=6001;mysql&gt; create table emp1 select * from emp;</code></pre><h3 id="查看kafka的消费者"><a href="#查看kafka的消费者" class="headerlink" title="查看kafka的消费者"></a>查看kafka的消费者</h3><pre><code class="json">//对应第一条insert语句{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;update&quot;,&quot;ts&quot;:1552097863,&quot;xid&quot;:89,&quot;commit&quot;:true,&quot;data&quot;:{&quot;empno&quot;:6001,&quot;ename&quot;:&quot;SIWA&quot;,&quot;job&quot;:&quot;DESIGNER&quot;,&quot;mgr&quot;:7001,&quot;hiredate&quot;:&quot;2019-03-08 00:00:00&quot;,&quot;sal&quot;:502.00,&quot;comm&quot;:6000.00,&quot;deptno&quot;:40},&quot;old&quot;:{&quot;sal&quot;:501.00}}//对应第二条insert语句{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;update&quot;,&quot;ts&quot;:1552097951,&quot;xid&quot;:123,&quot;commit&quot;:true,&quot;data&quot;:{&quot;empno&quot;:6001,&quot;ename&quot;:&quot;SIWA&quot;,&quot;job&quot;:&quot;DESIGNER&quot;,&quot;mgr&quot;:7001,&quot;hiredate&quot;:&quot;2019-03-08 00:00:00&quot;,&quot;sal&quot;:603.00,&quot;comm&quot;:6000.00,&quot;deptno&quot;:40},&quot;old&quot;:{&quot;sal&quot;:502.00}}//对应建表，相当于在新表emp1里插入了emp表里的所有数据{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:0,&quot;data&quot;:{&quot;empno&quot;:6001,&quot;ename&quot;:&quot;SIWA&quot;,&quot;job&quot;:&quot;DESIGNER&quot;,&quot;mgr&quot;:7001,&quot;hiredate&quot;:&quot;2019-03-08 00:00:00&quot;,&quot;sal&quot;:603.00,&quot;comm&quot;:6000.00,&quot;deptno&quot;:40}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:1,&quot;data&quot;:{&quot;empno&quot;:7369,&quot;ename&quot;:&quot;SMITH&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7902,&quot;hiredate&quot;:&quot;1980-12-17 00:00:00&quot;,&quot;sal&quot;:800.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:2,&quot;data&quot;:{&quot;empno&quot;:7499,&quot;ename&quot;:&quot;ALLEN&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-02-20 00:00:00&quot;,&quot;sal&quot;:1600.00,&quot;comm&quot;:300.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:3,&quot;data&quot;:{&quot;empno&quot;:7521,&quot;ename&quot;:&quot;WARD&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-02-22 00:00:00&quot;,&quot;sal&quot;:1250.00,&quot;comm&quot;:500.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:4,&quot;data&quot;:{&quot;empno&quot;:7566,&quot;ename&quot;:&quot;JONES&quot;,&quot;job&quot;:&quot;MANAGER&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1981-04-02 00:00:00&quot;,&quot;sal&quot;:2975.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:5,&quot;data&quot;:{&quot;empno&quot;:7654,&quot;ename&quot;:&quot;MARTIN&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-09-28 00:00:00&quot;,&quot;sal&quot;:1250.00,&quot;comm&quot;:1400.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:6,&quot;data&quot;:{&quot;empno&quot;:7698,&quot;ename&quot;:&quot;BLAKE&quot;,&quot;job&quot;:&quot;MANAGER&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1981-05-01 00:00:00&quot;,&quot;sal&quot;:2850.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:7,&quot;data&quot;:{&quot;empno&quot;:7782,&quot;ename&quot;:&quot;CLARK&quot;,&quot;job&quot;:&quot;MANAGER&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1981-06-09 00:00:00&quot;,&quot;sal&quot;:2450.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:10}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:8,&quot;data&quot;:{&quot;empno&quot;:7788,&quot;ename&quot;:&quot;SCOTT&quot;,&quot;job&quot;:&quot;ANALYST&quot;,&quot;mgr&quot;:7566,&quot;hiredate&quot;:&quot;1987-04-19 00:00:00&quot;,&quot;sal&quot;:3000.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:9,&quot;data&quot;:{&quot;empno&quot;:7839,&quot;ename&quot;:&quot;KING&quot;,&quot;job&quot;:&quot;PRESIDENT&quot;,&quot;mgr&quot;:0,&quot;hiredate&quot;:&quot;1981-11-17 00:00:00&quot;,&quot;sal&quot;:5000.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:10}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:10,&quot;data&quot;:{&quot;empno&quot;:7844,&quot;ename&quot;:&quot;TURNER&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-09-08 00:00:00&quot;,&quot;sal&quot;:1500.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:11,&quot;data&quot;:{&quot;empno&quot;:7876,&quot;ename&quot;:&quot;ADAMS&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7788,&quot;hiredate&quot;:&quot;1987-05-23 00:00:00&quot;,&quot;sal&quot;:1100.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:12,&quot;data&quot;:{&quot;empno&quot;:7900,&quot;ename&quot;:&quot;JAMES&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-12-03 00:00:00&quot;,&quot;sal&quot;:950.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:13,&quot;data&quot;:{&quot;empno&quot;:7902,&quot;ename&quot;:&quot;FORD&quot;,&quot;job&quot;:&quot;ANALYST&quot;,&quot;mgr&quot;:7566,&quot;hiredate&quot;:&quot;1981-12-03 00:00:00&quot;,&quot;sal&quot;:3000.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;xoffset&quot;:14,&quot;data&quot;:{&quot;empno&quot;:7934,&quot;ename&quot;:&quot;MILLER&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7782,&quot;hiredate&quot;:&quot;1982-01-23 00:00:00&quot;,&quot;sal&quot;:1300.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:10}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552098958,&quot;xid&quot;:435,&quot;commit&quot;:true,&quot;data&quot;:{&quot;empno&quot;:8888,&quot;ename&quot;:&quot;HIVE&quot;,&quot;job&quot;:&quot;PROGRAM&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1988-01-23 00:00:00&quot;,&quot;sal&quot;:10300.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:null}}# 数据已经正常同步到kafka中</code></pre><h3 id="Maxwell的过滤功能"><a href="#Maxwell的过滤功能" class="headerlink" title="Maxwell的过滤功能"></a>Maxwell的过滤功能</h3><p>参考过滤配置： &lt;<a href="http://maxwells-daemon.io/filtering/%3E" target="_blank" rel="noopener">http://maxwells-daemon.io/filtering/%3E</a>;</p><pre><code class="shell">[root@hadoop001 maxwell-1.17.1]# bin/maxwell --user=&#39;maxwell&#39; --password=&#39;maxwell&#39; \--host=&#39;127.0.0.1&#39; --filter &#39;exclude: *.*, include:hlwtest.emp1&#39; \--producer=kafka --kafka_version=0.10.0.1 --kafka.bootstrap.servers=192.168.137.2:9092 --kafka_topic=maxwell--filter &#39;exclude: *.*, include:hlwtest.emp1&#39;的意思是只监控hlwtest.emp1表的变化，其他的都不监控//MySQL中update数据mysql&gt; update emp set sal=730 where empno=6001;mysql&gt; update emp1 set sal=330 where empno=6001;mysql&gt; update emp set sal=730 where empno=6001;mysql&gt; update emp1 set sal=331 where empno=6001;//Kafka消费者接收到的数据{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;update&quot;,&quot;ts&quot;:1552099858,&quot;xid&quot;:916,&quot;commit&quot;:true,&quot;data&quot;:{&quot;empno&quot;:6001,&quot;ename&quot;:&quot;SIWA&quot;,&quot;job&quot;:&quot;DESIGNER&quot;,&quot;mgr&quot;:7001,&quot;hiredate&quot;:&quot;2019-03-08 00:00:00&quot;,&quot;sal&quot;:330.00,&quot;comm&quot;:6000.00,&quot;deptno&quot;:40},&quot;old&quot;:{&quot;sal&quot;:321.00}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp1&quot;,&quot;type&quot;:&quot;update&quot;,&quot;ts&quot;:1552099858,&quot;xid&quot;:922,&quot;commit&quot;:true,&quot;data&quot;:{&quot;empno&quot;:6001,&quot;ename&quot;:&quot;SIWA&quot;,&quot;job&quot;:&quot;DESIGNER&quot;,&quot;mgr&quot;:7001,&quot;hiredate&quot;:&quot;2019-03-08 00:00:00&quot;,&quot;sal&quot;:331.00,&quot;comm&quot;:6000.00,&quot;deptno&quot;:40},&quot;old&quot;:{&quot;sal&quot;:330.00}}确实只消费到了emp1表的update语句，而没有接收到emp表的更新</code></pre><h3 id="Maxwell-的bootstrap"><a href="#Maxwell-的bootstrap" class="headerlink" title="Maxwell 的bootstrap"></a>Maxwell 的bootstrap</h3><pre><code class="sql">mysql&gt; insert into maxwell.bootstrap (database_name, table_name) values (&quot;hlwtest&quot;, &quot;emp&quot;);mysql&gt; select * from maxwell.bootstrap;+----+---------------+------------+--------------+-------------+---------------+------------+------------+---------------------+---------------------+------------------+-----------------+-----------+| id | database_name | table_name | where_clause | is_complete | inserted_rows | total_rows | created_at | started_at          | completed_at        | binlog_file      | binlog_position | client_id |+----+---------------+------------+--------------+-------------+---------------+------------+------------+---------------------+---------------------+------------------+-----------------+-----------+|  1 | hlwtest       | emp        | NULL         |           1 |            16 |          0 | NULL       | 2019-03-09 11:33:11 | 2019-03-09 11:33:11 | mysql-bin.000018 |          225498 | maxwell   |+----+---------------+------------+--------------+-------------+---------------+------------+------------+---------------------+---------------------+------------------+-----------------+-----------+</code></pre><h3 id="kafka的消费窗口"><a href="#kafka的消费窗口" class="headerlink" title="kafka的消费窗口"></a>kafka的消费窗口</h3><pre><code class="json">{&quot;database&quot;:&quot;maxwell&quot;,&quot;table&quot;:&quot;bootstrap&quot;,&quot;type&quot;:&quot;insert&quot;,&quot;ts&quot;:1552102248,&quot;xid&quot;:1555,&quot;commit&quot;:true,&quot;data&quot;:{&quot;id&quot;:1,&quot;database_name&quot;:&quot;hlwtest&quot;,&quot;table_name&quot;:&quot;emp&quot;,&quot;where_clause&quot;:null,&quot;is_complete&quot;:0,&quot;inserted_rows&quot;:0,&quot;total_rows&quot;:0,&quot;created_at&quot;:null,&quot;started_at&quot;:null,&quot;completed_at&quot;:null,&quot;binlog_file&quot;:null,&quot;binlog_position&quot;:0,&quot;client_id&quot;:&quot;maxwell&quot;}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-start&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:6001,&quot;ename&quot;:&quot;SIWA&quot;,&quot;job&quot;:&quot;DESIGNER&quot;,&quot;mgr&quot;:7001,&quot;hiredate&quot;:&quot;2019-03-08 00:00:00&quot;,&quot;sal&quot;:730.00,&quot;comm&quot;:6000.00,&quot;deptno&quot;:40}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7369,&quot;ename&quot;:&quot;SMITH&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7902,&quot;hiredate&quot;:&quot;1980-12-17 00:00:00&quot;,&quot;sal&quot;:800.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7499,&quot;ename&quot;:&quot;ALLEN&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-02-20 00:00:00&quot;,&quot;sal&quot;:1600.00,&quot;comm&quot;:300.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7521,&quot;ename&quot;:&quot;WARD&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-02-22 00:00:00&quot;,&quot;sal&quot;:1250.00,&quot;comm&quot;:500.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7566,&quot;ename&quot;:&quot;JONES&quot;,&quot;job&quot;:&quot;MANAGER&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1981-04-02 00:00:00&quot;,&quot;sal&quot;:2975.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7654,&quot;ename&quot;:&quot;MARTIN&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-09-28 00:00:00&quot;,&quot;sal&quot;:1250.00,&quot;comm&quot;:1400.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7698,&quot;ename&quot;:&quot;BLAKE&quot;,&quot;job&quot;:&quot;MANAGER&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1981-05-01 00:00:00&quot;,&quot;sal&quot;:2850.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7782,&quot;ename&quot;:&quot;CLARK&quot;,&quot;job&quot;:&quot;MANAGER&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1981-06-09 00:00:00&quot;,&quot;sal&quot;:2450.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:10}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7788,&quot;ename&quot;:&quot;SCOTT&quot;,&quot;job&quot;:&quot;ANALYST&quot;,&quot;mgr&quot;:7566,&quot;hiredate&quot;:&quot;1987-04-19 00:00:00&quot;,&quot;sal&quot;:3000.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7839,&quot;ename&quot;:&quot;KING&quot;,&quot;job&quot;:&quot;PRESIDENT&quot;,&quot;mgr&quot;:0,&quot;hiredate&quot;:&quot;1981-11-17 00:00:00&quot;,&quot;sal&quot;:5000.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:10}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7844,&quot;ename&quot;:&quot;TURNER&quot;,&quot;job&quot;:&quot;SALESMAN&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-09-08 00:00:00&quot;,&quot;sal&quot;:1500.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7876,&quot;ename&quot;:&quot;ADAMS&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7788,&quot;hiredate&quot;:&quot;1987-05-23 00:00:00&quot;,&quot;sal&quot;:1100.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7900,&quot;ename&quot;:&quot;JAMES&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7698,&quot;hiredate&quot;:&quot;1981-12-03 00:00:00&quot;,&quot;sal&quot;:950.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:30}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7902,&quot;ename&quot;:&quot;FORD&quot;,&quot;job&quot;:&quot;ANALYST&quot;,&quot;mgr&quot;:7566,&quot;hiredate&quot;:&quot;1981-12-03 00:00:00&quot;,&quot;sal&quot;:3000.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:20}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:7934,&quot;ename&quot;:&quot;MILLER&quot;,&quot;job&quot;:&quot;CLERK&quot;,&quot;mgr&quot;:7782,&quot;hiredate&quot;:&quot;1982-01-23 00:00:00&quot;,&quot;sal&quot;:1300.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:10}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-insert&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{&quot;empno&quot;:8888,&quot;ename&quot;:&quot;HIVE&quot;,&quot;job&quot;:&quot;PROGRAM&quot;,&quot;mgr&quot;:7839,&quot;hiredate&quot;:&quot;1988-01-23 00:00:00&quot;,&quot;sal&quot;:10300.00,&quot;comm&quot;:0.00,&quot;deptno&quot;:null}}{&quot;database&quot;:&quot;maxwell&quot;,&quot;table&quot;:&quot;bootstrap&quot;,&quot;type&quot;:&quot;update&quot;,&quot;ts&quot;:1552102391,&quot;xid&quot;:1617,&quot;commit&quot;:true,&quot;data&quot;:{&quot;id&quot;:1,&quot;database_name&quot;:&quot;hlwtest&quot;,&quot;table_name&quot;:&quot;emp&quot;,&quot;where_clause&quot;:null,&quot;is_complete&quot;:1,&quot;inserted_rows&quot;:16,&quot;total_rows&quot;:0,&quot;created_at&quot;:null,&quot;started_at&quot;:&quot;2019-03-09 11:33:11&quot;,&quot;completed_at&quot;:&quot;2019-03-09 11:33:11&quot;,&quot;binlog_file&quot;:&quot;mysql-bin.000018&quot;,&quot;binlog_position&quot;:225498,&quot;client_id&quot;:&quot;maxwell&quot;},&quot;old&quot;:{&quot;is_complete&quot;:0,&quot;inserted_rows&quot;:1,&quot;completed_at&quot;:null}}{&quot;database&quot;:&quot;hlwtest&quot;,&quot;table&quot;:&quot;emp&quot;,&quot;type&quot;:&quot;bootstrap-complete&quot;,&quot;ts&quot;:1552102391,&quot;data&quot;:{}}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> MySQL </tag>
            
            <tag> Maxwell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生产SparkStreaming数据零丢失最佳实践(含代码)</title>
      <link href="/2019/11/23/%E7%94%9F%E4%BA%A7SparkStreaming%E6%95%B0%E6%8D%AE%E9%9B%B6%E4%B8%A2%E5%A4%B1%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E4%BB%A3%E7%A0%81)/"/>
      <url>/2019/11/23/%E7%94%9F%E4%BA%A7SparkStreaming%E6%95%B0%E6%8D%AE%E9%9B%B6%E4%B8%A2%E5%A4%B1%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E4%BB%A3%E7%A0%81)/</url>
      
        <content type="html"><![CDATA[<h2 id="MySQL创建存储offset的表格"><a href="#MySQL创建存储offset的表格" class="headerlink" title="MySQL创建存储offset的表格"></a>MySQL创建存储offset的表格</h2><pre><code class="sql">mysql&gt; use testmysql&gt; create table hlw_offset(        topic varchar(32),        groupid varchar(50),        partitions int,        fromoffset bigint,        untiloffset bigint,        primary key(topic,groupid,partitions)        );</code></pre><h2 id="Maven依赖包"><a href="#Maven依赖包" class="headerlink" title="Maven依赖包"></a>Maven依赖包</h2><pre><code class="xml">&lt;scala.version&gt;2.11.8&lt;/scala.version&gt;&lt;spark.version&gt;2.3.1&lt;/spark.version&gt;&lt;scalikejdbc.version&gt;2.5.0&lt;/scalikejdbc.version&gt;--------------------------------------------------&lt;dependency&gt;    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;    &lt;version&gt;${scala.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;    &lt;version&gt;${spark.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;    &lt;version&gt;${spark.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;    &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;    &lt;version&gt;${spark.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;    &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt;    &lt;version&gt;${spark.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;mysql&lt;/groupId&gt;    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;    &lt;version&gt;5.1.27&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.scalikejdbc/scalikejdbc --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt;    &lt;artifactId&gt;scalikejdbc_2.11&lt;/artifactId&gt;    &lt;version&gt;2.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt;    &lt;artifactId&gt;scalikejdbc-config_2.11&lt;/artifactId&gt;    &lt;version&gt;2.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;com.typesafe&lt;/groupId&gt;    &lt;artifactId&gt;config&lt;/artifactId&gt;    &lt;version&gt;1.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;    &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;    &lt;version&gt;3.5&lt;/version&gt;&lt;/dependency&gt;</code></pre><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><ol><li>StreamingContext</li><li>从kafka中获取数据(从外部存储获取offset–&gt;根据offset获取kafka中的数据)</li><li>根据业务进行逻辑处理</li><li>将处理结果存到外部存储中–保存offset</li><li>启动程序，等待程序结束</li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ol><li><p>SparkStreaming主体代码如下</p><pre><code class="scala">import kafka.common.TopicAndPartitionimport kafka.message.MessageAndMetadataimport kafka.serializer.StringDecoderimport org.apache.spark.SparkConfimport org.apache.spark.streaming.kafka.{HasOffsetRanges, KafkaUtils}import org.apache.spark.streaming.{Seconds, StreamingContext}import scalikejdbc._import scalikejdbc.config._object JDBCOffsetApp { def main(args: Array[String]): Unit = {   //创建SparkStreaming入口   val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;JDBCOffsetApp&quot;)   val ssc = new StreamingContext(conf,Seconds(5))   //kafka消费主题   val topics = ValueUtils.getStringValue(&quot;kafka.topics&quot;).split(&quot;,&quot;).toSet   //kafka参数   //这里应用了自定义的ValueUtils工具类，来获取application.conf里的参数，方便后期修改   val kafkaParams = Map[String,String](     &quot;metadata.broker.list&quot;-&gt;ValueUtils.getStringValue(&quot;metadata.broker.list&quot;),     &quot;auto.offset.reset&quot;-&gt;ValueUtils.getStringValue(&quot;auto.offset.reset&quot;),     &quot;group.id&quot;-&gt;ValueUtils.getStringValue(&quot;group.id&quot;)   )   //先使用scalikejdbc从MySQL数据库中读取offset信息   //+------------+------------------+------------+------------+-------------+   //| topic      | groupid          | partitions | fromoffset | untiloffset |   //+------------+------------------+------------+------------+-------------+   //MySQL表结构如上，将“topic”，“partitions”，“untiloffset”列读取出来   //组成 fromOffsets: Map[TopicAndPartition, Long]，后面createDirectStream用到   DBs.setup()   val fromOffset = DB.readOnly( implicit session =&gt; {     SQL(&quot;select * from hlw_offset&quot;).map(rs =&gt; {       (TopicAndPartition(rs.string(&quot;topic&quot;),rs.int(&quot;partitions&quot;)),rs.long(&quot;untiloffset&quot;))     }).list().apply()   }).toMap   //如果MySQL表中没有offset信息，就从0开始消费；如果有，就从已经存在的offset开始消费     val messages = if (fromOffset.isEmpty) {       println(&quot;从头开始消费...&quot;)       KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder](ssc,kafkaParams,topics)     } else {       println(&quot;从已存在记录开始消费...&quot;)       val messageHandler = (mm:MessageAndMetadata[String,String]) =&gt; (mm.key(),mm.message())       KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder,(String,String)](ssc,kafkaParams,fromOffset,messageHandler)     }     messages.foreachRDD(rdd=&gt;{       if(!rdd.isEmpty()){         //输出rdd的数据量         println(&quot;数据统计记录为：&quot;+rdd.count())         //官方案例给出的获得rdd offset信息的方法，offsetRanges是由一系列offsetRange组成的数组//          trait HasOffsetRanges {//            def offsetRanges: Array[OffsetRange]//          }         val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges         offsetRanges.foreach(x =&gt; {           //输出每次消费的主题，分区，开始偏移量和结束偏移量           println(s&quot;---${x.topic},${x.partition},${x.fromOffset},${x.untilOffset}---&quot;)          //将最新的偏移量信息保存到MySQL表中           DB.autoCommit( implicit session =&gt; {             SQL(&quot;replace into hlw_offset(topic,groupid,partitions,fromoffset,untiloffset) values (?,?,?,?,?)&quot;)           .bind(x.topic,ValueUtils.getStringValue(&quot;group.id&quot;),x.partition,x.fromOffset,x.untilOffset)             .update().apply()           })         })       }     })   ssc.start()   ssc.awaitTermination() }}</code></pre></li><li><p>自定义的ValueUtils工具类如下</p><pre><code class="scala">import com.typesafe.config.ConfigFactoryimport org.apache.commons.lang3.StringUtilsobject ValueUtils {val load = ConfigFactory.load() def getStringValue(key:String, defaultValue:String=&quot;&quot;) = {val value = load.getString(key)   if(StringUtils.isNotEmpty(value)) {     value   } else {     defaultValue   } }}</code></pre></li><li><p>application.conf内容如下</p><pre><code class="properties">metadata.broker.list = &quot;192.168.137.251:9092&quot;auto.offset.reset = &quot;smallest&quot;group.id = &quot;hlw_offset_group&quot;kafka.topics = &quot;hlw_offset&quot;serializer.class = &quot;kafka.serializer.StringEncoder&quot;request.required.acks = &quot;1&quot;# JDBC settingsdb.default.driver = &quot;com.mysql.jdbc.Driver&quot;db.default.url=&quot;jdbc:mysql://hadoop000:3306/test&quot;db.default.user=&quot;root&quot;db.default.password=&quot;123456&quot;</code></pre></li><li><p>自定义kafka producer</p><pre><code class="scala">import java.util.{Date, Properties}import kafka.producer.{KeyedMessage, Producer, ProducerConfig}object KafkaProducer { def main(args: Array[String]): Unit = {   val properties = new Properties()   properties.put(&quot;serializer.class&quot;,ValueUtils.getStringValue(&quot;serializer.class&quot;))   properties.put(&quot;metadata.broker.list&quot;,ValueUtils.getStringValue(&quot;metadata.broker.list&quot;))   properties.put(&quot;request.required.acks&quot;,ValueUtils.getStringValue(&quot;request.required.acks&quot;))   val producerConfig = new ProducerConfig(properties)   val producer = new Producer[String,String](producerConfig)   val topic = ValueUtils.getStringValue(&quot;kafka.topics&quot;)   //每次产生100条数据   var i = 0   for (i &lt;- 1 to 100) {     val runtimes = new Date().toString    val messages = new KeyedMessage[String, String](topic,i+&quot;&quot;,&quot;hlw: &quot;+runtimes)     producer.send(messages)   }   println(&quot;数据发送完毕...&quot;) }}</code></pre></li></ol><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ol><li><p>启动kafka服务，并创建主题</p><pre><code class="shell">[hadoop@hadoop000 bin]$ ./kafka-server-start.sh -daemon /home/hadoop/app/kafka_2.11-0.10.0.1/config/server.properties[hadoop@hadoop000 bin]$ ./kafka-topics.sh --list --zookeeper localhost:2181/kafka[hadoop@hadoop000 bin]$ ./kafka-topics.sh --create --zookeeper localhost:2181/kafka --replication-factor 1 --partitions 1 --topic hlw_offset</code></pre></li><li><p>测试前查看MySQL中offset表，刚开始是个空表</p><pre><code class="sql">mysql&gt; select * from hlw_offset;Empty set (0.00 sec)</code></pre></li></ol><ol start="3"><li><p>通过kafka producer产生500条数据</p></li><li><p>启动SparkStreaming程序</p><pre><code class="java">//控制台输出结果：从头开始消费...数据统计记录为：500---hlw_offset,0,0,500---</code></pre></li><li><p>查看MySQL表，offset记录成功</p><pre><code class="sql">mysql&gt; select * from hlw_offset;+------------+------------------+------------+------------+-------------+| topic      | groupid          | partitions | fromoffset | untiloffset |+------------+------------------+------------+------------+-------------+| hlw_offset | hlw_offset_group |          0 |          0 |         500 |+------------+------------------+------------+------------+-------------+</code></pre></li><li><p>关闭SparkStreaming程序，再使用kafka producer生产300条数据,再次启动spark程序（如果spark从500开始消费，说明成功读取了offset，做到了只读取一次语义）</p><pre><code>//控制台结果输出：从已存在记录开始消费...数据统计记录为：300---hlw_offset,0,500,800---</code></pre></li><li><p>查看更新后的offset MySQL数据</p><pre><code class="sql">mysql&gt; select * from hlw_offset;+------------+------------------+------------+------------+-------------+| topic      | groupid          | partitions | fromoffset | untiloffset |+------------+------------------+------------+------------+-------------+| hlw_offset | hlw_offset_group |          0 |        500 |         800 |+------------+------------------+------------+------------+-------------+</code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Spark Streaming </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
